{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mario-rot/Introduction-to-Human-Language-Technology/blob/main/se6_LaurenTucker_MarioRosas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lab session 6 (Word Sense Disambiguation) - ILTH\n",
    "\n",
    "**Students:** Lauren Tucker & Mario Rosas"
   ],
   "metadata": {
    "id": "LpEp-jhafvsx"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0LXucmalIYk"
   },
   "source": [
    "## Paraphrases Template"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install wget"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfZdkJaMZeHO",
    "outputId": "51b24e90-b343-4d76-bad1-27ad422e1409"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=957eba1f5bed7595b484f865e9f3901b5d4cc9ffdab30160a0803a29220d6e50\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# import wget\n",
    "# URL = \"https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz\"\n",
    "# response = wget.download(URL, \"test_gold.tgz\")"
   ],
   "metadata": {
    "id": "Ow-SCj4EXL21"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# importing the \"tarfile\" module\n",
    "import tarfile\n",
    "\n",
    "def unzip_tars(root, files, targetFolder = None):\n",
    "  for fileN in files:\n",
    "    file = tarfile.open(root + fileN)# open file\n",
    "    if targetFolder:\n",
    "      file.extractall(targetFolder) # extracting file\n",
    "    else:\n",
    "      file.extractall(root) # extracting file\n",
    "    file.close()"
   ],
   "metadata": {
    "id": "vE2DAJT-9xo_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# unzip_tars('/content/', ['test_gold.tgz'], '/content/')"
   ],
   "metadata": {
    "id": "7zTBIq_k_3Yb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mi9UgnrHx1NN"
   },
   "source": [
    "import pandas as pd"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fx90zvqXx5eA"
   },
   "source": [
    "dt = pd.read_csv('Complementary Material/test-gold/STS.input.SMTeuroparl.txt',sep='\\t',header=None)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0veZLO1xmGCD",
    "outputId": "e54eeca1-30b2-4f92-a4d2-c79d2cfc138b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    }
   },
   "source": [
    "dt.head()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   0  \\\n0  The leaders have now been given a new chance a...   \n1  Amendment No 7 proposes certain changes in the...   \n2  Let me remind you that our allies include ferv...   \n3        The vote will take place today at 5.30 p.m.   \n4  The fishermen are inactive, tired and disappoi...   \n\n                                                   1  \n0  The leaders benefit aujourd' hui of a new luck...  \n1  Amendment No 7 is proposing certain changes in...  \n2  I would like to remind you that among our alli...  \n3                 The vote will take place at 5.30pm  \n4  The fishermen are inactive, tired and disappoi...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The leaders have now been given a new chance a...</td>\n      <td>The leaders benefit aujourd' hui of a new luck...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Amendment No 7 proposes certain changes in the...</td>\n      <td>Amendment No 7 is proposing certain changes in...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Let me remind you that our allies include ferv...</td>\n      <td>I would like to remind you that among our alli...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The vote will take place today at 5.30 p.m.</td>\n      <td>The vote will take place at 5.30pm</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The fishermen are inactive, tired and disappoi...</td>\n      <td>The fishermen are inactive, tired and disappoi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4xWDSCAD0mZ_"
   },
   "source": [
    "dt['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cLYgKtW0cL-",
    "outputId": "b8983694-3122-43b6-e566-b88572d9cdae"
   },
   "source": [
    "dt.shape"
   ],
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "(459, 3)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rDhoyDeY0Xpv",
    "outputId": "f9c2afd1-6a28-4134-e2d2-3d5cb8222f87"
   },
   "source": [
    "dt.head()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   0  \\\n0  The leaders have now been given a new chance a...   \n1  Amendment No 7 proposes certain changes in the...   \n2  Let me remind you that our allies include ferv...   \n3        The vote will take place today at 5.30 p.m.   \n4  The fishermen are inactive, tired and disappoi...   \n\n                                                   1    gs  \n0  The leaders benefit aujourd' hui of a new luck...  4.50  \n1  Amendment No 7 is proposing certain changes in...  5.00  \n2  I would like to remind you that among our alli...  4.25  \n3                 The vote will take place at 5.30pm  4.50  \n4  The fishermen are inactive, tired and disappoi...  5.00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>gs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The leaders have now been given a new chance a...</td>\n      <td>The leaders benefit aujourd' hui of a new luck...</td>\n      <td>4.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Amendment No 7 proposes certain changes in the...</td>\n      <td>Amendment No 7 is proposing certain changes in...</td>\n      <td>5.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Let me remind you that our allies include ferv...</td>\n      <td>I would like to remind you that among our alli...</td>\n      <td>4.25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The vote will take place today at 5.30 p.m.</td>\n      <td>The vote will take place at 5.30pm</td>\n      <td>4.50</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The fishermen are inactive, tired and disappoi...</td>\n      <td>The fishermen are inactive, tired and disappoi...</td>\n      <td>5.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Excercise"
   ],
   "metadata": {
    "id": "lW6chj7pBMrs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_signs(wrd):\n",
    "  wrd = list(wrd)\n",
    "  wrd = [word for word in wrd if not any(caracter in signs for caracter in word)]\n",
    "  wrd = ''.join(wrd)\n",
    "  return wrd\n",
    "\n",
    "def clean(corpus, stopwords, minwords_len, signs):\n",
    "   corpus = corpus.split(' ')\n",
    "   corpus = [word.lower() for word in corpus]\n",
    "   corpus = [word if not any(caracter in signs for caracter in word) else remove_signs(word) for word in corpus]\n",
    "   corpus = [word for word in corpus if word not in stopwords and word.isalpha()]\n",
    "   corpus = [word for word in corpus if len(word) > minwords_len]\n",
    "   return corpus\n",
    "\n",
    "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
    "pos_map = {'N': NOUN,\n",
    "           'V':VERB,\n",
    "           'J':ADJ,\n",
    "           'R':ADV}\n",
    "\n",
    "# corpus = list(gutenberg.words('blake-poems.txt'))\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "signs = string.punctuation\n",
    "minwords_len = 2"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yi0VomaR6RKR",
    "outputId": "8ed85713-26f5-472f-c550-4d1703ce5609"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cleaned = clean(dt.loc[0][0], stopwords, minwords_len, signs)\n",
    "cleaned"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjciS3FadwuA",
    "outputId": "c2b365ef-18d9-44d8-b9b1-4ce85eedada6"
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "['leaders', 'given', 'new', 'chance', 'let', 'hope', 'seize']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "accepted_pos = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "pos_map = {'n': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "           'v': ['VB', 'VBD', 'VBN', 'VBP', 'VBZ'],\n",
    "           'j': ['JJ', 'JJR', 'JJS'],\n",
    "           'r': ['RB', 'RBR', 'RBS']}\n",
    "correcting = {'n':'n', 'v':'v', 'j':'a', 'r':'r'}\n",
    "pos_map2 = {'N': NOUN,\n",
    "           'V':VERB,\n",
    "           'J':ADJ,\n",
    "           'R':ADV}\n",
    "\n",
    "\n",
    "def filter_pos(pair):\n",
    "    if pair[1][0].lower() in list(pos_map.keys()):\n",
    "        return pair[0], correcting[pair[1][0].lower()]\n",
    "    return None, None\n",
    "\n",
    "# def filter_pos(pair):\n",
    "#     if pair[1] in accepted_pos:\n",
    "#         return pair\n",
    "#     return None, None"
   ],
   "metadata": {
    "id": "30lVcaC8eRnZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def applying_lesk(sentence):\n",
    "    cleaned = clean(sentence, stopwords, minwords_len, signs)\n",
    "    pairs = nltk.pos_tag(cleaned)\n",
    "    synsets = []\n",
    "    for pair in pairs:\n",
    "      context = cleaned\n",
    "      word, pos = filter_pos(pair)\n",
    "      if pos:\n",
    "        synset = nltk.wsd.lesk(context, word, pos)\n",
    "      else:\n",
    "          synset = False\n",
    "      if synset:\n",
    "        synsets.append(synset)\n",
    "    return synsets\n"
   ],
   "metadata": {
    "id": "Smm8f-ZHfhRG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(p):\n",
    "    if p[1][0] in {'N', 'V', 'J', 'R'}:\n",
    "        return wnl.lemmatize(p[0].lower(), pos=pos_map2[p[1][0]])\n",
    "    return p[0]\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    cleaned = clean(sentence, stopwords, minwords_len, signs)\n",
    "    tagged = nltk.pos_tag(cleaned)\n",
    "    return [[lemmatize(pair),pair[1]] for pair in tagged]\n",
    "\n",
    "\n",
    "def applying_lesk_lemmas(sentence):\n",
    "    context = [i[0] for i in sentence]\n",
    "    synsets = []\n",
    "    for pair in sentence:\n",
    "        context = cleaned\n",
    "        word, pos = filter_pos(pair)\n",
    "        if pos:\n",
    "            synset = nltk.wsd.lesk(context, word, pos)\n",
    "        else:\n",
    "            synset = False\n",
    "        if synset:\n",
    "            synsets.append(synset)\n",
    "    return synsets"
   ],
   "metadata": {
    "id": "MWA8f6ukWb2U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_metric(metric, elements): # Fenction to calculate Jaccard Distance\n",
    "    if metric == 'jaccard':\n",
    "        res = jaccard_distance(set(applying_lesk(elements[0])),\n",
    "                               set(applying_lesk(elements[1])))\n",
    "    if metric == 'jaccard_lemmas':\n",
    "      res = jaccard_distance(set(applying_lesk_lemmas(lemmatize_sentence(elements[0]))),\n",
    "                             set(applying_lesk_lemmas(lemmatize_sentence(elements[1]))))\n",
    "    return res"
   ],
   "metadata": {
    "id": "TYColBS56ee3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dt['jaccard'],dt['jaccard_lemmas'] = \"\",\"\""
   ],
   "metadata": {
    "id": "5A-e0Eey8H8U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(len(dt)): # Iterating to get Jaccard distances over each column of the dataframe\n",
    "  dt['jaccard'][i] = np.float64(compute_metric('jaccard',[dt.iloc[i][0],dt.iloc[i][1]]))*10\n",
    "  dt['jaccard_lemmas'][i] = np.float64(compute_metric('jaccard_lemmas',[dt.iloc[i][0],dt.iloc[i][1]]))*10"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EO9hPVN_6-B5",
    "outputId": "c8d575e0-e695-4757-854b-109fe40dbb33"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_52980\\607086207.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dt['jaccard'][i] = np.float64(compute_metric('jaccard',[dt.iloc[i][0],dt.iloc[i][1]]))*10\n",
      "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_52980\\607086207.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dt['jaccard_lemmas'][i] = np.float64(compute_metric('jaccard_lemmas',[dt.iloc[i][0],dt.iloc[i][1]]))*10\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     0  \\\n0    The leaders have now been given a new chance a...   \n1    Amendment No 7 proposes certain changes in the...   \n2    Let me remind you that our allies include ferv...   \n3          The vote will take place today at 5.30 p.m.   \n4    The fishermen are inactive, tired and disappoi...   \n..                                                 ...   \n454  It is our job to continue to support Latvia wi...   \n455        The vote will take place today at 5.30 p.m.   \n456  Neither was there a qualified majority within ...   \n457  Let me remind you that our allies include ferv...   \n458  We often pontificate here about being the repr...   \n\n                                                     1     gs   jaccard  \\\n0    The leaders benefit aujourd' hui of a new luck...  4.500  5.555556   \n1    Amendment No 7 is proposing certain changes in...  5.000       0.0   \n2    I would like to remind you that among our alli...  4.250       7.5   \n3                   The vote will take place at 5.30pm  4.500       2.5   \n4    The fishermen are inactive, tired and disappoi...  5.000       0.0   \n..                                                 ...    ...       ...   \n454  It is of our duty of continue to support the c...  5.000  6.363636   \n455                   Vote will take place at 17 h 30.  4.750       2.5   \n456  There was no qualified majority in this Parlia...  5.000       5.0   \n457  I hold you recall that our allies, there are e...  4.000       7.5   \n458  We often take pride here to represent the citi...  3.833       5.0   \n\n    jaccard_lemmas  \n0         5.555556  \n1              0.0  \n2              7.5  \n3              2.5  \n4              0.0  \n..             ...  \n454       6.363636  \n455            2.5  \n456            5.0  \n457            7.5  \n458            5.0  \n\n[459 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>gs</th>\n      <th>jaccard</th>\n      <th>jaccard_lemmas</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The leaders have now been given a new chance a...</td>\n      <td>The leaders benefit aujourd' hui of a new luck...</td>\n      <td>4.500</td>\n      <td>5.555556</td>\n      <td>5.555556</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Amendment No 7 proposes certain changes in the...</td>\n      <td>Amendment No 7 is proposing certain changes in...</td>\n      <td>5.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Let me remind you that our allies include ferv...</td>\n      <td>I would like to remind you that among our alli...</td>\n      <td>4.250</td>\n      <td>7.5</td>\n      <td>7.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The vote will take place today at 5.30 p.m.</td>\n      <td>The vote will take place at 5.30pm</td>\n      <td>4.500</td>\n      <td>2.5</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The fishermen are inactive, tired and disappoi...</td>\n      <td>The fishermen are inactive, tired and disappoi...</td>\n      <td>5.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>454</th>\n      <td>It is our job to continue to support Latvia wi...</td>\n      <td>It is of our duty of continue to support the c...</td>\n      <td>5.000</td>\n      <td>6.363636</td>\n      <td>6.363636</td>\n    </tr>\n    <tr>\n      <th>455</th>\n      <td>The vote will take place today at 5.30 p.m.</td>\n      <td>Vote will take place at 17 h 30.</td>\n      <td>4.750</td>\n      <td>2.5</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>456</th>\n      <td>Neither was there a qualified majority within ...</td>\n      <td>There was no qualified majority in this Parlia...</td>\n      <td>5.000</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>457</th>\n      <td>Let me remind you that our allies include ferv...</td>\n      <td>I hold you recall that our allies, there are e...</td>\n      <td>4.000</td>\n      <td>7.5</td>\n      <td>7.5</td>\n    </tr>\n    <tr>\n      <th>458</th>\n      <td>We often pontificate here about being the repr...</td>\n      <td>We often take pride here to represent the citi...</td>\n      <td>3.833</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>459 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ],
   "metadata": {
    "id": "DHxnMG2kWb2U",
    "outputId": "19936ff0-5f8c-4f66-91a4-19ea8513fdac"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pearsonr(dt['gs'], 1-dt['jaccard']) # Calculating the pearson correlation between GS results and 1-Jaccard calculated data with lesk algorithm applied"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0LVZunA82f2",
    "outputId": "29838d82-4474-4270-e923-157406d661af"
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "PearsonRResult(statistic=0.454977041340358, pvalue=7.827296967470474e-25)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "PearsonRResult(statistic=0.4565851262924598, pvalue=5.110934549149753e-25)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(dt['gs'], 1-dt['jaccard_lemmas']) # Calculating the pearson correlation between GS results and 1-Jaccard calculated data with lesk algorithm + lemmatizer applied"
   ],
   "metadata": {
    "id": "hM-D_q43Wb2V",
    "outputId": "0ecd3291-a508-47a3-80f9-880b31819c5c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 --> 7.777777777777778 -- 6.25\n",
      "14 --> 10.0 -- 8.75\n",
      "62 --> 10.0 -- 8.75\n",
      "80 --> 10.0 -- 8.75\n",
      "100 --> 7.0 -- 5.555555555555555\n",
      "118 --> 8.0 -- 6.666666666666666\n",
      "175 --> 10.0 -- 5.0\n",
      "184 --> 5.555555555555555 -- 3.75\n",
      "187 --> 5.7142857142857135 -- 3.333333333333333\n",
      "188 --> 6.25 -- 4.285714285714286\n",
      "209 --> 6.666666666666666 -- 0.0\n",
      "235 --> 7.777777777777778 -- 6.25\n",
      "236 --> 10.0 -- 8.88888888888889\n",
      "242 --> 7.0 -- 5.555555555555555\n",
      "255 --> 8.0 -- 6.666666666666666\n",
      "256 --> 7.0 -- 5.555555555555555\n",
      "265 --> 6.666666666666666 -- 5.0\n",
      "285 --> 2.8571428571428568 -- 0.0\n",
      "287 --> 10.0 -- 5.0\n",
      "323 --> 7.777777777777778 -- 6.25\n",
      "343 --> 5.0 -- 2.8571428571428568\n",
      "347 --> 6.25 -- 4.285714285714286\n",
      "404 --> 10.0 -- 8.88888888888889\n",
      "410 --> 4.285714285714286 -- 1.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for index, (j,jl) in enumerate(zip(dt['jaccard'],dt['jaccard_lemmas'])):\n",
    "    if j != jl:\n",
    "        count += 1\n",
    "        print(index,'-->', j,'--',jl)"
   ],
   "metadata": {
    "id": "qrZE51CDWb2V",
    "outputId": "7e7cfc89-d410-424f-ea67-ad4223dca1a9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "For reference, the Pearson correlation for Lab 2 was:\n",
    "\n",
    "PearsonRResult(statistic=0.45049771693186835, pvalue=2.5356459143049236e-24)\n",
    "\n",
    "and the Pearson correlation for Lab 3 was:\n",
    "\n",
    "PearsonRResult(statistic=0.48102317341708245, pvalue=5.904510415498371e-28)\n",
    "\n",
    "\n",
    "In this lab, we first removed stopwords and punctuation from our original sentences and filtered with pos_tag to receive pairs of significant words and their corresponding parts of speech for all nouns, adjectives, adverbs, and verbs. We then applied the lesk algorithm to this cleaned set of words and received a Pearson correlation value of:\n",
    "\n",
    "PearsonRResult(statistic=0.454977041340358, pvalue=7.827296967470474e-25)\n",
    "\n",
    "We then took an additional step and lemmatized the cleaned set of words and performed lesk on this lemmatized set, and we received a Pearson correlation value of:\n",
    "\n",
    "PearsonRResult(statistic=0.4565851262924598, pvalue=5.110934549149753e-25)\n",
    "\n",
    "\n",
    "Lesk, both when performed on the lemmatized set and the unlemmatized set, performed better than the document analysis from Lab 2. In both cases, this is likely because the context that the Lesk algorithm uses is supporting the evaluation of word similarities and creates a more accurate picture of the meaning of each word. Lesk occasionally performed better on the lemmatized set than on the unlemmatized set, with 24 of the instances having lower Jaccard coefficients for the lemmatized sets. Contrary to our expectations, the lemmatization does not seem to have a negative impact on the context. In other words, lemmatizing the sentence does not strip any meaning from the context. In any case, however, the improvement in performance when using Lesk is very small, despite our predictions that the improvement would be much greater, since we hypothesized that having context would improve performance.\n",
    "\n",
    "\n",
    "Lesk, both when performed on the lemmatized set and the unlemmatized set, performed worse than just lemmatizing the words as done in Lab 3, as shown above. One possible reason for this may be that, when taking more words into account when determining similarity (i.e. by adding the context), there is more room for error. If the wrong synset is determined for some word in the context, then it will impact the interpreted meaning of the whole sentence. With just lemmatizing, on the other hand, as done in Lab 3, there is less chance of error, since only the structure of each word is being changed and examined, without the subjective interpretations of meaning used with context analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "MjCFQFyeeO28"
   }
  }
 ]
}
