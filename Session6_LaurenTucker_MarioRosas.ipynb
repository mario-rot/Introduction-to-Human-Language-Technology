{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mario-rot/Introduction-to-Human-Language-Technology/blob/main/Session6_LaurenTucker_MarioRosas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEp-jhafvsx"
      },
      "source": [
        "# Lab session 6 (Word Sense Disambiguation) - ILTH\n",
        "\n",
        "**Students:** Lauren Tucker & Mario Rosas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0LXucmalIYk"
      },
      "source": [
        "## Paraphrases Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zTBIq_k_3Yb",
        "outputId": "d8982d6f-e836-46e8-b57c-e4bf8f7adc66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Introduction-to-Human-Language-Technology'...\n",
            "remote: Enumerating objects: 849, done.\u001b[K\n",
            "remote: Counting objects: 100% (849/849), done.\u001b[K\n",
            "remote: Compressing objects: 100% (777/777), done.\u001b[K\n",
            "remote: Total 849 (delta 107), reused 788 (delta 67), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (849/849), 2.25 MiB | 17.73 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "git clone https://github.com/mario-rot/Introduction-to-Human-Language-Technology.git\n",
        "cd 'Introduction-to-Human-Language-Technology'\n",
        "mv 'Complementary Material' /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mi9UgnrHx1NN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fx90zvqXx5eA"
      },
      "outputs": [],
      "source": [
        "dt = pd.read_csv('Complementary Material/test-gold/STS.input.SMTeuroparl.txt',sep='\\t',header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0veZLO1xmGCD",
        "outputId": "723788a4-ab08-48dd-d0d4-64fce377a121"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The leaders have now been given a new chance a...</td>\n",
              "      <td>The leaders benefit aujourd' hui of a new luck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Amendment No 7 proposes certain changes in the...</td>\n",
              "      <td>Amendment No 7 is proposing certain changes in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Let me remind you that our allies include ferv...</td>\n",
              "      <td>I would like to remind you that among our alli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The vote will take place today at 5.30 p.m.</td>\n",
              "      <td>The vote will take place at 5.30pm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  \\\n",
              "0  The leaders have now been given a new chance a...   \n",
              "1  Amendment No 7 proposes certain changes in the...   \n",
              "2  Let me remind you that our allies include ferv...   \n",
              "3        The vote will take place today at 5.30 p.m.   \n",
              "4  The fishermen are inactive, tired and disappoi...   \n",
              "\n",
              "                                                   1  \n",
              "0  The leaders benefit aujourd' hui of a new luck...  \n",
              "1  Amendment No 7 is proposing certain changes in...  \n",
              "2  I would like to remind you that among our alli...  \n",
              "3                 The vote will take place at 5.30pm  \n",
              "4  The fishermen are inactive, tired and disappoi...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4xWDSCAD0mZ_"
      },
      "outputs": [],
      "source": [
        "dt['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cLYgKtW0cL-",
        "outputId": "ad1ed731-4756-4b39-ab31-055a7b133bc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(459, 3)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rDhoyDeY0Xpv",
        "outputId": "fb044fa2-3235-4def-8ccc-a13b8b3cc0b7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>gs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The leaders have now been given a new chance a...</td>\n",
              "      <td>The leaders benefit aujourd' hui of a new luck...</td>\n",
              "      <td>4.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Amendment No 7 proposes certain changes in the...</td>\n",
              "      <td>Amendment No 7 is proposing certain changes in...</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Let me remind you that our allies include ferv...</td>\n",
              "      <td>I would like to remind you that among our alli...</td>\n",
              "      <td>4.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The vote will take place today at 5.30 p.m.</td>\n",
              "      <td>The vote will take place at 5.30pm</td>\n",
              "      <td>4.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  \\\n",
              "0  The leaders have now been given a new chance a...   \n",
              "1  Amendment No 7 proposes certain changes in the...   \n",
              "2  Let me remind you that our allies include ferv...   \n",
              "3        The vote will take place today at 5.30 p.m.   \n",
              "4  The fishermen are inactive, tired and disappoi...   \n",
              "\n",
              "                                                   1    gs  \n",
              "0  The leaders benefit aujourd' hui of a new luck...  4.50  \n",
              "1  Amendment No 7 is proposing certain changes in...  5.00  \n",
              "2  I would like to remind you that among our alli...  4.25  \n",
              "3                 The vote will take place at 5.30pm  4.50  \n",
              "4  The fishermen are inactive, tired and disappoi...  5.00  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW6chj7pBMrs"
      },
      "source": [
        "# Excercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi0VomaR6RKR",
        "outputId": "eafeb3ab-9645-4079-9990-d1e3aca37689"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.metrics import jaccard_distance\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "import collections\n",
        "import nltk\n",
        "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.text import Text\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_signs(wrd):\n",
        "  wrd = list(wrd)\n",
        "  wrd = [word for word in wrd if not any(caracter in signs for caracter in word)]\n",
        "  wrd = ''.join(wrd)\n",
        "  return wrd\n",
        "\n",
        "def clean(corpus, stopwords, minwords_len, signs):\n",
        "   corpus = corpus.split(' ')\n",
        "   corpus = [word.lower() for word in corpus]\n",
        "   corpus = [word if not any(caracter in signs for caracter in word) else remove_signs(word) for word in corpus]\n",
        "   corpus = [word for word in corpus if word not in stopwords and word.isalpha()]\n",
        "   corpus = [word for word in corpus if len(word) > minwords_len]\n",
        "   return corpus\n",
        "\n",
        "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
        "pos_map = {'N': NOUN,\n",
        "           'V':VERB,\n",
        "           'J':ADJ,\n",
        "           'R':ADV}\n",
        "\n",
        "# corpus = list(gutenberg.words('blake-poems.txt'))\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "signs = string.punctuation\n",
        "minwords_len = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjciS3FadwuA",
        "outputId": "fd0ae6ba-25d9-4936-f421-e67e86619fb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['leaders', 'given', 'new', 'chance', 'let', 'hope', 'seize']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned = clean(dt.loc[0][0], stopwords, minwords_len, signs)\n",
        "cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "30lVcaC8eRnZ"
      },
      "outputs": [],
      "source": [
        "accepted_pos = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
        "\n",
        "pos_map = {'n': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
        "           'v': ['VB', 'VBD', 'VBN', 'VBP', 'VBZ'],\n",
        "           'j': ['JJ', 'JJR', 'JJS'],\n",
        "           'r': ['RB', 'RBR', 'RBS']}\n",
        "correcting = {'n':'n', 'v':'v', 'j':'a', 'r':'r'}\n",
        "pos_map2 = {'N': NOUN,\n",
        "           'V':VERB,\n",
        "           'J':ADJ,\n",
        "           'R':ADV}\n",
        "\n",
        "\n",
        "def filter_pos(pair):\n",
        "    if pair[1][0].lower() in list(pos_map.keys()):\n",
        "        return pair[0], correcting[pair[1][0].lower()]\n",
        "    return None, None\n",
        "\n",
        "# def filter_pos(pair):\n",
        "#     if pair[1] in accepted_pos:\n",
        "#         return pair\n",
        "#     return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Smm8f-ZHfhRG"
      },
      "outputs": [],
      "source": [
        "def applying_lesk(sentence):\n",
        "    cleaned = clean(sentence, stopwords, minwords_len, signs)\n",
        "    pairs = nltk.pos_tag(cleaned)\n",
        "    synsets = []\n",
        "    for pair in pairs:\n",
        "      context = cleaned\n",
        "      word, pos = filter_pos(pair)\n",
        "      if pos:\n",
        "        synset = nltk.wsd.lesk(context, word, pos)\n",
        "      else:\n",
        "          synset = False\n",
        "      if synset:\n",
        "        synsets.append(synset)\n",
        "    return synsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "MWA8f6ukWb2U"
      },
      "outputs": [],
      "source": [
        "wnl = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def lemmatize(p):\n",
        "    if p[1][0] in {'N', 'V', 'J', 'R'}:\n",
        "        return wnl.lemmatize(p[0].lower(), pos=pos_map2[p[1][0]])\n",
        "    return p[0]\n",
        "\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    cleaned = clean(sentence, stopwords, minwords_len, signs)\n",
        "    tagged = nltk.pos_tag(cleaned)\n",
        "    return [[lemmatize(pair),pair[1]] for pair in tagged]\n",
        "\n",
        "\n",
        "def applying_lesk_lemmas(sentence):\n",
        "    context = [i[0] for i in sentence]\n",
        "    synsets = []\n",
        "    for pair in sentence:\n",
        "        context = cleaned\n",
        "        word, pos = filter_pos(pair)\n",
        "        if pos:\n",
        "            synset = nltk.wsd.lesk(context, word, pos)\n",
        "        else:\n",
        "            synset = False\n",
        "        if synset:\n",
        "            synsets.append(synset)\n",
        "    return synsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TYColBS56ee3"
      },
      "outputs": [],
      "source": [
        "def compute_metric(metric, elements): # Fenction to calculate Jaccard Distance\n",
        "    if metric == 'jaccard':\n",
        "        res = jaccard_distance(set(applying_lesk(elements[0])),\n",
        "                               set(applying_lesk(elements[1])))\n",
        "    if metric == 'jaccard_lemmas':\n",
        "      res = jaccard_distance(set(applying_lesk_lemmas(lemmatize_sentence(elements[0]))),\n",
        "                             set(applying_lesk_lemmas(lemmatize_sentence(elements[1]))))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5A-e0Eey8H8U"
      },
      "outputs": [],
      "source": [
        "dt['jaccard'],dt['jaccard_lemmas'] = \"\",\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO9hPVN_6-B5",
        "outputId": "0fc49803-1c43-45cb-ba36-0a85753fd471"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_12320\\3921866450.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dt['jaccard'][i] = np.float64(compute_metric('jaccard',[dt.iloc[i][0],dt.iloc[i][1]]))*10\n",
            "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_12320\\3921866450.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dt['jaccard_lemmas'][i] = np.float64(compute_metric('jaccard_lemmas',[dt.iloc[i][0],dt.iloc[i][1]]))*10\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(dt)): # Iterating to get Jaccard distances over each column of the dataframe\n",
        "  dt['jaccard'][i] = np.float64(compute_metric('jaccard',[dt.iloc[i][0],dt.iloc[i][1]]))*10\n",
        "  dt['jaccard_lemmas'][i] = np.float64(compute_metric('jaccard_lemmas',[dt.iloc[i][0],dt.iloc[i][1]]))*10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DHxnMG2kWb2U",
        "outputId": "5d1c0881-d26b-4374-cad3-9bc010d07d6d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>gs</th>\n",
              "      <th>jaccard</th>\n",
              "      <th>jaccard_lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The leaders have now been given a new chance a...</td>\n",
              "      <td>The leaders benefit aujourd' hui of a new luck...</td>\n",
              "      <td>4.500</td>\n",
              "      <td>5.555556</td>\n",
              "      <td>5.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Amendment No 7 proposes certain changes in the...</td>\n",
              "      <td>Amendment No 7 is proposing certain changes in...</td>\n",
              "      <td>5.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Let me remind you that our allies include ferv...</td>\n",
              "      <td>I would like to remind you that among our alli...</td>\n",
              "      <td>4.250</td>\n",
              "      <td>7.5</td>\n",
              "      <td>7.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The vote will take place today at 5.30 p.m.</td>\n",
              "      <td>The vote will take place at 5.30pm</td>\n",
              "      <td>4.500</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>5.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>It is our job to continue to support Latvia wi...</td>\n",
              "      <td>It is of our duty of continue to support the c...</td>\n",
              "      <td>5.000</td>\n",
              "      <td>6.363636</td>\n",
              "      <td>6.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>The vote will take place today at 5.30 p.m.</td>\n",
              "      <td>Vote will take place at 17 h 30.</td>\n",
              "      <td>4.750</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>Neither was there a qualified majority within ...</td>\n",
              "      <td>There was no qualified majority in this Parlia...</td>\n",
              "      <td>5.000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>Let me remind you that our allies include ferv...</td>\n",
              "      <td>I hold you recall that our allies, there are e...</td>\n",
              "      <td>4.000</td>\n",
              "      <td>7.5</td>\n",
              "      <td>7.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>We often pontificate here about being the repr...</td>\n",
              "      <td>We often take pride here to represent the citi...</td>\n",
              "      <td>3.833</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>459 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     0  \\\n",
              "0    The leaders have now been given a new chance a...   \n",
              "1    Amendment No 7 proposes certain changes in the...   \n",
              "2    Let me remind you that our allies include ferv...   \n",
              "3          The vote will take place today at 5.30 p.m.   \n",
              "4    The fishermen are inactive, tired and disappoi...   \n",
              "..                                                 ...   \n",
              "454  It is our job to continue to support Latvia wi...   \n",
              "455        The vote will take place today at 5.30 p.m.   \n",
              "456  Neither was there a qualified majority within ...   \n",
              "457  Let me remind you that our allies include ferv...   \n",
              "458  We often pontificate here about being the repr...   \n",
              "\n",
              "                                                     1     gs   jaccard  \\\n",
              "0    The leaders benefit aujourd' hui of a new luck...  4.500  5.555556   \n",
              "1    Amendment No 7 is proposing certain changes in...  5.000       0.0   \n",
              "2    I would like to remind you that among our alli...  4.250       7.5   \n",
              "3                   The vote will take place at 5.30pm  4.500       2.5   \n",
              "4    The fishermen are inactive, tired and disappoi...  5.000       0.0   \n",
              "..                                                 ...    ...       ...   \n",
              "454  It is of our duty of continue to support the c...  5.000  6.363636   \n",
              "455                   Vote will take place at 17 h 30.  4.750       2.5   \n",
              "456  There was no qualified majority in this Parlia...  5.000       5.0   \n",
              "457  I hold you recall that our allies, there are e...  4.000       7.5   \n",
              "458  We often take pride here to represent the citi...  3.833       5.0   \n",
              "\n",
              "    jaccard_lemmas  \n",
              "0         5.555556  \n",
              "1              0.0  \n",
              "2              7.5  \n",
              "3              2.5  \n",
              "4              0.0  \n",
              "..             ...  \n",
              "454       6.363636  \n",
              "455            2.5  \n",
              "456            5.0  \n",
              "457            7.5  \n",
              "458            5.0  \n",
              "\n",
              "[459 rows x 5 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0LVZunA82f2",
        "outputId": "b3ef7f3b-4094-4724-a23e-29641dbe8e01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=0.454977041340358, pvalue=7.827296967470474e-25)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'], 1-dt['jaccard']) # Calculating the pearson correlation between GS results and 1-Jaccard calculated data with lesk algorithm applied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM-D_q43Wb2V",
        "outputId": "229ddded-f00e-4c28-9f30-ac6a9e478668"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=0.4565851262924598, pvalue=5.110934549149753e-25)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'], 1-dt['jaccard_lemmas']) # Calculating the pearson correlation between GS results and 1-Jaccard calculated data with lesk algorithm + lemmatizer applied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrZE51CDWb2V",
        "outputId": "61d1f576-5c4b-43fa-d7b0-3972e29f09e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8 --> 7.777777777777778 -- 6.25\n",
            "14 --> 10.0 -- 8.75\n",
            "62 --> 10.0 -- 8.75\n",
            "80 --> 10.0 -- 8.75\n",
            "100 --> 7.0 -- 5.555555555555555\n",
            "118 --> 8.0 -- 6.666666666666666\n",
            "175 --> 10.0 -- 5.0\n",
            "184 --> 5.555555555555555 -- 3.75\n",
            "187 --> 5.7142857142857135 -- 3.333333333333333\n",
            "188 --> 6.25 -- 4.285714285714286\n",
            "209 --> 6.666666666666666 -- 0.0\n",
            "235 --> 7.777777777777778 -- 6.25\n",
            "236 --> 10.0 -- 8.88888888888889\n",
            "242 --> 7.0 -- 5.555555555555555\n",
            "255 --> 8.0 -- 6.666666666666666\n",
            "256 --> 7.0 -- 5.555555555555555\n",
            "265 --> 6.666666666666666 -- 5.0\n",
            "285 --> 2.8571428571428568 -- 0.0\n",
            "287 --> 10.0 -- 5.0\n",
            "323 --> 7.777777777777778 -- 6.25\n",
            "343 --> 5.0 -- 2.8571428571428568\n",
            "347 --> 6.25 -- 4.285714285714286\n",
            "404 --> 10.0 -- 8.88888888888889\n",
            "410 --> 4.285714285714286 -- 1.6666666666666665\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for index, (j,jl) in enumerate(zip(dt['jaccard'],dt['jaccard_lemmas'])):\n",
        "    if j != jl:\n",
        "        count += 1\n",
        "        print(index,'-->', j,'--',jl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCFQFyeeO28"
      },
      "source": [
        "# Conclusion\n",
        "For reference, the Pearson correlation for Lab 2 was:\n",
        "\n",
        "*PearsonRResult(statistic=0.45049771693186835, pvalue=2.5356459143049236e-24)*\n",
        "\n",
        "and the Pearson correlation for Lab 3 was:\n",
        "\n",
        "*PearsonRResult(statistic=0.48102317341708245, pvalue=5.904510415498371e-28)*\n",
        "\n",
        "\n",
        "In this lab, we first removed stopwords and punctuation from our original sentences and filtered with pos_tag to receive pairs of significant words and their corresponding parts of speech for all nouns, adjectives, adverbs, and verbs. We then applied the lesk algorithm to this cleaned set of words and received a Pearson correlation value of:\n",
        "\n",
        "*PearsonRResult(statistic=0.454977041340358, pvalue=7.827296967470474e-25)*\n",
        "\n",
        "We then took an additional step and lemmatized the cleaned set of words and performed lesk on this lemmatized set, and we received a Pearson correlation value of:\n",
        "\n",
        "*PearsonRResult(statistic=0.4565851262924598, pvalue=5.110934549149753e-25)*\n",
        "\n",
        "\n",
        "Lesk, both when performed on the lemmatized set and the unlemmatized set, performed better than the document analysis from Lab 2. In both cases, this is likely because the context that the Lesk algorithm uses is supporting the evaluation of word similarities and creates a more accurate picture of the meaning of each word. Lesk occasionally performed better on the lemmatized set than on the unlemmatized set, with 24 of the instances having lower Jaccard coefficients for the lemmatized sets. Contrary to our expectations, the lemmatization does not seem to have a negative impact on the context. In other words, lemmatizing the sentence does not strip any meaning from the context. In any case, however, the improvement in performance when using Lesk is very small, despite our predictions that the improvement would be much greater, since we hypothesized that having context would improve performance.\n",
        "\n",
        "\n",
        "Lesk, both when performed on the lemmatized set and the unlemmatized set, performed worse than just lemmatizing the words as done in Lab 3, as shown above. One possible reason for this may be that, when taking more words into account when determining similarity (i.e. by adding the context), there is more room for error. If the wrong synset is determined for some word in the context, then it will impact the interpreted meaning of the whole sentence. With just lemmatizing, on the other hand, as done in Lab 3, there is less chance of error, since only the structure of each word is being changed and examined, without the subjective interpretations of meaning used with context analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "def applying_lesk(sentence, all = False):\n",
        "    context = sentence.split(' ')\n",
        "    cleaned = clean(sentence, stopwords, minwords_len, signs)\n",
        "    pairs = nltk.pos_tag(cleaned)\n",
        "    synsets = []\n",
        "    for pair in pairs:\n",
        "    #   context = cleaned\n",
        "        word, pos = filter_pos(pair)\n",
        "        if pos:\n",
        "            synset = nltk.wsd.lesk(context, word, pos)\n",
        "        else:\n",
        "            synset = False\n",
        "        if synset:\n",
        "            synsets.append(synset.lemmas()[0].name())\n",
        "        elif not synset and all:\n",
        "            synsets.append(pair[0])\n",
        "    return synsets\n",
        "\n",
        "wnl = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def lemmatize(p):\n",
        "    if p[1][0] in {'N', 'V', 'J', 'R'}:\n",
        "        return wnl.lemmatize(p[0].lower(), pos=pos_map2[p[1][0]])\n",
        "    return p[0]\n",
        "\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    cleaned = clean(sentence, stopwords, minwords_len, signs)\n",
        "    tagged = nltk.pos_tag(cleaned)\n",
        "    return [[lemmatize(pair),pair[1]] for pair in tagged]\n",
        "\n",
        "\n",
        "def applying_lesk_lemmas(sentence, all = False):\n",
        "    context = sentence.split(' ')\n",
        "    lemmatized_sentence = lemmatize_sentence(sentence)\n",
        "    synsets = []\n",
        "    for pair in lemmatized_sentence:\n",
        "        # context = cleaned\n",
        "        word, pos = filter_pos(pair)\n",
        "        if pos:\n",
        "            synset = nltk.wsd.lesk(context, word, pos)\n",
        "        else:\n",
        "            synset = False\n",
        "        if synset:\n",
        "            synsets.append(synset.lemmas()[0].name())\n",
        "        elif not synset and all:\n",
        "            synsets.append(pair[0])\n",
        "    return synsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The',\n",
              " 'leaders',\n",
              " 'have',\n",
              " 'now',\n",
              " 'been',\n",
              " 'given',\n",
              " 'a',\n",
              " 'new',\n",
              " 'chance',\n",
              " 'and',\n",
              " 'let',\n",
              " 'us',\n",
              " 'hope',\n",
              " 'they',\n",
              " 'seize',\n",
              " 'it',\n",
              " '.']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.word_tokenize(dt[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['leader', 'NNS'],\n",
              " ['give', 'VBN'],\n",
              " ['new', 'JJ'],\n",
              " ['chance', 'NN'],\n",
              " ['let', 'VBD'],\n",
              " ['hope', 'PRP'],\n",
              " ['seize', 'VB']]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemmatize_sentence(dt[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'split'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\mario\\Github\\UPC\\IHLT - Introduction to Human Language Technology\\Session6_LaurenTucker_MarioRosas.ipynb Celda 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Session6_LaurenTucker_MarioRosas.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m applying_lesk_lemmas(lemmatize_sentence(dt[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]))\n",
            "\u001b[1;32mc:\\Users\\mario\\Github\\UPC\\IHLT - Introduction to Human Language Technology\\Session6_LaurenTucker_MarioRosas.ipynb Celda 28\u001b[0m in \u001b[0;36mapplying_lesk_lemmas\u001b[1;34m(sentence, all)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Session6_LaurenTucker_MarioRosas.ipynb#X36sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapplying_lesk_lemmas\u001b[39m(sentence, \u001b[39mall\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Session6_LaurenTucker_MarioRosas.ipynb#X36sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     context \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Session6_LaurenTucker_MarioRosas.ipynb#X36sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     lemmatized_sentence \u001b[39m=\u001b[39m lemmatize_sentence(sentence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Session6_LaurenTucker_MarioRosas.ipynb#X36sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     synsets \u001b[39m=\u001b[39m []\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "applying_lesk_lemmas(lemmatize_sentence(dt[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filter_pos(nltk.pos_tag(clean('neither', stopwords, minwords_len, signs))[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'savings_bank'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
        "\n",
        "synset = nltk.wsd.lesk(context, 'bank', 'n')\n",
        "\n",
        "synset.lemmas()[0].name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I went to the bank to deposit money .'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sen = ' '.join(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'])\n",
        "sen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['survive', 'savings_bank', 'down_payment', 'money']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "applying_lesk_lemmas(sen, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['survive', 'savings_bank', 'down_payment', 'money']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "applying_lesk(sen, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I went to the bank to deposit money .'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.word_tokenize(sen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('went', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('the', 'DT'),\n",
              " ('bank', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('deposit', 'VB'),\n",
              " ('money', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.pos_tag(nltk.word_tokenize(sen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def applying_lesk(sentence, all = False):\n",
        "    # context = sentence.split(' ')\n",
        "    cleaned = nltk.word_tokenize(sentence)\n",
        "    pairs = nltk.pos_tag(cleaned)\n",
        "    synsets = []\n",
        "    for pair in pairs:\n",
        "    #   context = cleaned\n",
        "        word, pos = filter_pos(pair)\n",
        "        if pos:\n",
        "            synset = nltk.wsd.lesk(cleaned, word, pos).lemmas()[0].name()\n",
        "        else:\n",
        "            synset = False\n",
        "        if synset:\n",
        "            synsets.append(synset)\n",
        "        elif not synset and all:\n",
        "            synsets.append(pair[0])\n",
        "    return synsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'survive', 'to', 'the', 'savings_bank', 'to', 'deposit', 'money', '.']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "applying_lesk(sen, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The leaders have now been given a new chance and let us hope they seize it.\n",
            "The leaders benefit aujourd' hui of a new luck and let's let them therefore seize it.\n"
          ]
        }
      ],
      "source": [
        "print(dt[0][0])\n",
        "print(dt[1][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['leadership', 'give', 'new', 'probability', 'let', 'assume']"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "applying_lesk(dt[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['leadership',\n",
              " 'profit',\n",
              " 'new',\n",
              " 'Lashkar-e-Taiba',\n",
              " 'let',\n",
              " 'therefore',\n",
              " 'impound']"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "applying_lesk(dt[1][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "wnl = nltk.stem.WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('aujourd', 'NN')]"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.pos_tag(['aujourd'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wn.synsets('aujourd', 'n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('IHLT')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0795eca24a98e58b2dcbec80c9554a91f94c5c7d4e675f06c8c2f85c434623a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
