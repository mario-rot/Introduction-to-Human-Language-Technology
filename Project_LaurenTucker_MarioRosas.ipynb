{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mario-rot/Introduction-to-Human-Language-Technology/blob/main/Project_LaurenTucker_MarioRosas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEp-jhafvsx"
      },
      "source": [
        "# Lab session 7 (Word Sequences) - ILTH\n",
        "\n",
        "**Students:** Lauren Tucker & Mario Rosas !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0LXucmalIYk"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zTBIq_k_3Yb",
        "outputId": "21414362-13fe-4e6a-8cc4-c4857505aa25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Introduction-to-Human-Language-Technology'...\n",
            "remote: Enumerating objects: 864, done.\u001b[K\n",
            "remote: Counting objects: 100% (864/864), done.\u001b[K\n",
            "remote: Compressing objects: 100% (792/792), done.\u001b[K\n",
            "remote: Total 864 (delta 117), reused 788 (delta 67), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (864/864), 2.27 MiB | 14.46 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting svgling\n",
            "  Downloading svgling-0.3.1-py3-none-any.whl (21 kB)\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.3.1 svgwrite-1.4.3\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "git clone https://github.com/mario-rot/Introduction-to-Human-Language-Technology.git\n",
        "cd 'Introduction-to-Human-Language-Technology'\n",
        "mv 'Complementary Material' /content/\n",
        "\n",
        "pip install svgling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mi9UgnrHx1NN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from text_processing import text_processing, compute_metrics\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "dt = pd.read_csv('Complementary Material/test-gold/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "dt['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW6chj7pBMrs"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading the data to the text processing class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uqvz4ye8TEVJ"
      },
      "outputs": [],
      "source": [
        "sentences_1 = dt[0]\n",
        "sentences_2 = dt[1]\n",
        "\n",
        "tp1 = text_processing(sentences_1)\n",
        "tp2 = text_processing(sentences_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.3971297709735512, pvalue=8.622653547988135e-19)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([sentences_1, sentences_2], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing with data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.4679440754704449, pvalue=2.358310094026997e-26)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.clean_data(), tp2.clean_data()], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing with tokenized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.45049771693186835, pvalue=2.5356459143049236e-24)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.tokenize_data(), tp2.tokenize_data()], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('----- First group ----',\n",
              " let            51\n",
              " report         51\n",
              " lose           51\n",
              " amendment      34\n",
              " power          34\n",
              "                ..\n",
              " others         17\n",
              " separate       17\n",
              " basis          17\n",
              " accumulated    17\n",
              " hatred         17\n",
              " Length: 136, dtype: int64,\n",
              " '----- Second group -----',\n",
              " report          45\n",
              " european        41\n",
              " take            38\n",
              " parliament      37\n",
              " president       32\n",
              "                 ..\n",
              " good             1\n",
              " alone            1\n",
              " offers           1\n",
              " country          1\n",
              " enthusiastic     1\n",
              " Length: 327, dtype: int64)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'----- First group ----',tp1.frequency(True, 'cleaned'),'----- Second group -----', tp2.frequency(True, 'cleaned')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('----- First group ----',\n",
              " .              391\n",
              " the            289\n",
              " ,              221\n",
              " of             153\n",
              " to             136\n",
              "               ... \n",
              " separate        17\n",
              " on              17\n",
              " basis           17\n",
              " accumulated     17\n",
              " hatred          17\n",
              " Length: 203, dtype: int64,\n",
              " '----- Second group -----',\n",
              " the             446\n",
              " .               390\n",
              " of              253\n",
              " to              205\n",
              " ,               144\n",
              "                ... \n",
              " country           1\n",
              " Vote              1\n",
              " h                 1\n",
              " enthusiastic      1\n",
              " about             1\n",
              " Length: 459, dtype: int64)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'----- First group ----',tp1.frequency(True, 'tokenized'),'----- Second group -----', tp2.frequency(True, 'tokenized')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing with lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### With cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.4810231734170824, pvalue=5.904510415498371e-28)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.lemmatize_data('cleaned'), tp2.lemmatize_data('cleaned')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### With tokenized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.45684718944780944, pvalue=4.766946098384746e-25)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.lemmatize_data('tokenized'), tp2.lemmatize_data('tokenized')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing with most common lemma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### With cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.4884451179547373, pvalue=6.780872227864517e-29)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.mc_lemmatize_data('cleaned'), tp2.mc_lemmatize_data('cleaned')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### With tokenized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.4598193578111567, pvalue=2.1538267683062985e-25)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.mc_lemmatize_data('tokenized'), tp2.mc_lemmatize_data('tokenized')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Testing with lesk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.46511878367859305, pvalue=5.1234758101231843e-26)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.apply_lesk_data('cleaned'), tp2.apply_lesk_data('cleaned')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.44025516061325287, pvalue=3.494129449862612e-23)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.apply_lesk_data('tokenized'), tp2.apply_lesk_data('tokenized')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Testing lesk with lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.46511878367859305, pvalue=5.1234758101231843e-26)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.apply_lesk_lemmas_data('cleaned'), tp2.apply_lesk_lemmas_data('cleaned')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.43751386820493765, pvalue=6.946178670452206e-23)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.apply_lesk_lemmas_data('tokenized'), tp2.apply_lesk_lemmas_data('tokenized')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing with name entities NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.4679440754704449, pvalue=2.358310094026997e-26)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.name_entities_nltk('cleaned'), tp2.name_entities_nltk('cleaned')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.4499943169285951, pvalue=2.8905921464511112e-24)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.name_entities_nltk('tokenized'), tp2.name_entities_nltk('tokenized')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Name entities NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.4493107049487957, pvalue=3.452252846323494e-24)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pearsonr(dt['gs'],compute_metrics([tp1.name_entities_spacy(), tp2.name_entities_spacy()], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing on training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from text_processing import text_processing, compute_metrics\n",
        "from scipy.stats import pearsonr\n",
        "from collections import Counter\n",
        "import io\n",
        "europarl = pd.read_csv('Complementary Material/train/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "europarl['gs'] = pd.read_csv('Complementary Material/train/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "\n",
        "vid = pd.read_csv('Complementary Material/train/STS.input.MSRvid.txt',sep='\\t',header=None)\n",
        "vid['gs'] = pd.read_csv('Complementary Material/train/STS.gs.MSRvid.txt',sep='\\t',header=None)\n",
        "\n",
        "with open('Complementary Material/train/STS.input.MSRpar.txt') as f:\n",
        "    lines = f.readlines()\n",
        "for index in range(len(lines)):\n",
        "    lines[index] = lines[index].replace('\\\"', ' ')\n",
        "par = pd.read_csv(io.StringIO(''.join(lines)), sep='\\t',header=None, on_bad_lines='warn')\n",
        "par['gs'] = pd.read_csv('Complementary Material/train/STS.gs.MSRpar.txt',sep='\\t',header=None)\n",
        "\n",
        "total = pd.concat([europarl, vid, par]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " -- Data hasn't been cleaned, to apply most_common_lemma to data is going to be cleaned with the default parameters --\n",
            "\n",
            "\n",
            " -- Data hasn't been cleaned, to apply most_common_lemma to data is going to be cleaned with the default parameters --\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.44013235092459513, pvalue=3.963249990137324e-36)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eur_tp1 = text_processing(europarl[0])\n",
        "eur_tp2 = text_processing(europarl[1])\n",
        "pearsonr(europarl['gs'],compute_metrics([eur_tp1.mc_lemmatize_data('cleaned'), eur_tp2.mc_lemmatize_data('cleaned')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " -- Data hasn't been cleaned, to apply most_common_lemma to data is going to be cleaned with the default parameters --\n",
            "\n",
            "\n",
            " -- Data hasn't been cleaned, to apply most_common_lemma to data is going to be cleaned with the default parameters --\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.743585243920414, pvalue=6.8783591678791514e-133)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vid_tp1 = text_processing(vid[0])\n",
        "vid_tp2 = text_processing(vid[1])\n",
        "pearsonr(vid['gs'],compute_metrics([vid_tp1.mc_lemmatize_data('cleaned'), vid_tp2.mc_lemmatize_data('cleaned')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " -- Data hasn't been cleaned, to apply most_common_lemma to data is going to be cleaned with the default parameters --\n",
            "\n",
            "\n",
            " -- Data hasn't been cleaned, to apply most_common_lemma to data is going to be cleaned with the default parameters --\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.4607370310110359, pvalue=1.1031499896752255e-40)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "par_tp1 = text_processing(par[0])\n",
        "par_tp2 = text_processing(par[1])\n",
        "pearsonr(par['gs'],compute_metrics([par_tp1.mc_lemmatize_data('cleaned'), par_tp2.mc_lemmatize_data('cleaned')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " -- Data hasn't been cleaned, to apply most_common_lemma to data is going to be cleaned with the default parameters --\n",
            "\n",
            "\n",
            " -- Data hasn't been cleaned, to apply most_common_lemma to data is going to be cleaned with the default parameters --\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.605690059448885, pvalue=8.285888569412453e-224)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_tp1 = text_processing(total[0])\n",
        "total_tp2 = text_processing(total[1])\n",
        "pearsonr(total['gs'],compute_metrics([total_tp1.mc_lemmatize_data('cleaned'), total_tp2.mc_lemmatize_data('cleaned')], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combining methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.5710296807912365, pvalue=1.5887971093998278e-193)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total1 = text_processing(total[0])\n",
        "total2 = text_processing(total[1])\n",
        "t1,t2 = total1.clean_data(), total2.clean_data()\n",
        "pearsonr(total['gs'],compute_metrics([t1, t2], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.5710296807912365, pvalue=1.5887971093998278e-193)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1,t2 = total_tp1.name_entities_nltk(data = t1), total_tp1.name_entities_nltk(data = t2)\n",
        "pearsonr(total['gs'],compute_metrics([t1, t2], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.6028411708465875, pvalue=3.5388242418854675e-221)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1,t2 = total_tp1.lemmatize_data(data = t1), total_tp1.lemmatize_data(data = t2)\n",
        "pearsonr(total['gs'],compute_metrics([t1, t2], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.6033214913285423, pvalue=1.2800280496346832e-221)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1,t2 = total_tp1.mc_lemmatize_data(data = t1), total_tp1.mc_lemmatize_data(data = t2)\n",
        "pearsonr(total['gs'],compute_metrics([t1, t2], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing in all test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from text_processing import text_processing, compute_metrics\n",
        "from scipy.stats import pearsonr\n",
        "from collections import Counter\n",
        "import io\n",
        "europarl = pd.read_csv('Complementary Material/test-gold/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "europarl['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "\n",
        "vid = pd.read_csv('Complementary Material/test-gold/STS.input.MSRvid.txt',sep='\\t',header=None)\n",
        "vid['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.MSRvid.txt',sep='\\t',header=None)\n",
        "\n",
        "with open('Complementary Material/test-gold/STS.input.MSRpar.txt', encoding='utf8') as f:\n",
        "    lines = f.readlines()\n",
        "for index in range(len(lines)):\n",
        "    lines[index] = lines[index].replace('\\\"', ' ')\n",
        "par = pd.read_csv(io.StringIO(''.join(lines)), sep='\\t',header=None, on_bad_lines='warn')\n",
        "par['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.MSRpar.txt',sep='\\t',header=None)\n",
        "\n",
        "onwn = pd.read_csv('Complementary Material/test-gold/STS.input.surprise.OnWN.txt',sep='\\t',header=None)\n",
        "onwn['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.surprise.OnWN.txt',sep='\\t',header=None)\n",
        "\n",
        "news = pd.read_csv('Complementary Material/test-gold/STS.input.surprise.SMTnews.txt',sep='\\t',header=None)\n",
        "news['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.surprise.SMTnews.txt',sep='\\t',header=None) \n",
        "\n",
        "total = pd.concat([europarl, vid, par, onwn, news]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.5453285641443748, pvalue=2.3585890733699138e-240)"
            ]
          },
          "execution_count": 336,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total1 = text_processing(total[0])\n",
        "total2 = text_processing(total[1])\n",
        "t1,t2 = total1.clean_data(), total2.clean_data()\n",
        "pearsonr(total['gs'],compute_metrics([t1, t2], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=-0.5768922741058112, pvalue=2.8794489176194942e-275)"
            ]
          },
          "execution_count": 337,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1,t2 = total_tp1.mc_lemmatize_data(data = t1), total_tp1.mc_lemmatize_data(data = t2)\n",
        "pearsonr(total['gs'],compute_metrics([t1, t2], ['jaccard']).do()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiments for extracting features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('The leaders have now been given a new chance and let us hope they seize it.',\n",
              " \"The leaders benefit aujourd' hui of a new luck and let's let them therefore seize it.\")"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total[0][0], total[1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [],
      "source": [
        "r1,r2 = total1.mc_lemmatize_data(lemma = True)[0],total2.mc_lemmatize_data(lemma = True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Lemma('leader.n.01.leader'),\n",
              " Lemma('give.v.01.give'),\n",
              " Lemma('new.a.01.new'),\n",
              " Lemma('opportunity.n.01.chance'),\n",
              " Lemma('let.v.01.let'),\n",
              " None,\n",
              " Lemma('seize.v.01.seize')]"
            ]
          },
          "execution_count": 205,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools as it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "combs = list(it.product(r1,r2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Lemma('leader.n.01.leader'),\n",
              " Lemma('give.v.01.give'),\n",
              " Lemma('new.a.01.new'),\n",
              " Lemma('opportunity.n.01.chance'),\n",
              " Lemma('let.v.01.let'),\n",
              " None,\n",
              " Lemma('seize.v.01.seize')]"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Lemma('leader.n.01.leader'),\n",
              " Lemma('profit.v.01.benefit'),\n",
              " None,\n",
              " None,\n",
              " Lemma('new.a.01.new'),\n",
              " Lemma('fortune.n.04.fortune'),\n",
              " Lemma('lashkar-e-taiba.n.01.Lashkar-e-Taiba'),\n",
              " Lemma('let.v.01.let'),\n",
              " Lemma('therefore.r.01.therefore'),\n",
              " Lemma('seize.v.01.seize')]"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "def path_similarities(lemmas1,lemmas2):\n",
        "    combs = list(it.product(lemmas1,lemmas2))\n",
        "    return [pair[0].synset().path_similarity(pair[1].synset()) for pair in combs if pair[0] and pair[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({1.0: 4,\n",
              "         0.1111111111111111: 7,\n",
              "         0.125: 9,\n",
              "         0.1: 6,\n",
              "         0.08333333333333333: 2,\n",
              "         0.25: 9,\n",
              "         0.3333333333333333: 7,\n",
              "         0.09090909090909091: 2,\n",
              "         0.16666666666666666: 1,\n",
              "         0.2: 1})"
            ]
          },
          "execution_count": 237,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(path_similarities(r1,r2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {},
      "outputs": [],
      "source": [
        "l1 = total1.lemmatize_data(r_pos_tag = False)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {},
      "outputs": [],
      "source": [
        "l2 = total2.lemmatize_data(r_pos_tag = False)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 447,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_synsets(lemma, tag):\n",
        "        if not tag:\n",
        "            synsets = wn.synsets(lemma)\n",
        "        else:\n",
        "            if lemma[1][0] in list(pos_map.keys()):\n",
        "                synsets = wn.synsets(lemma[0], pos_map[lemma[1][0]])\n",
        "            else:\n",
        "                synsets = wn.synsets(lemma[0])\n",
        "        return synsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 448,
      "metadata": {},
      "outputs": [],
      "source": [
        "def path_similarity_word(lemma1,lemma2,tag = False):\n",
        "    synsets1 = get_synsets(lemma1, tag)\n",
        "    synsets2 = get_synsets(lemma2, tag)\n",
        "    similarities = [syn1.path_similarity(syn2) for syn1 in synsets2 for syn2 in synsets2]\n",
        "    if similarities != []:\n",
        "        return max(similarities)\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['leader', 'give', 'new', 'chance', 'let', 'hope', 'seize'],\n",
              " ['leader',\n",
              "  'benefit',\n",
              "  'aujourd',\n",
              "  'hui',\n",
              "  'new',\n",
              "  'luck',\n",
              "  'let',\n",
              "  'let',\n",
              "  'therefore',\n",
              "  'seize'])"
            ]
          },
          "execution_count": 441,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l1,l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 450,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path_similarity_word(l1[0],l2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {},
      "outputs": [],
      "source": [
        "def path_similarity_sentence(lemmas1, lemmas2, tag = False):\n",
        "    if tag:\n",
        "        lemmas1 = nltk.pos_tag(lemmas1)\n",
        "        lemmas2 = nltk.pos_tag(lemmas2)\n",
        "    mean1 = sum(max([path_similarity_word(l1,l2, tag) for l2 in lemmas2]) for l1 in lemmas1)/len(lemmas1)\n",
        "    mean2 = sum(max([path_similarity_word(l2,l1, tag) for l1 in lemmas1]) for l2 in lemmas2)/len(lemmas2)\n",
        "    if mean1 != 0 and mean2 != 0:\n",
        "        return (2*mean1*mean2)/(mean1+mean2)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 457,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path_similarity_sentence(l1,l2,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 465,
      "metadata": {},
      "outputs": [],
      "source": [
        "def path_similarity(data = False, tag = False):\n",
        "    # j_data = self.data if not data else data\n",
        "    j_data = data\n",
        "    result = []\n",
        "    for n,row in enumerate(j_data):\n",
        "        if n%50 == 0:\n",
        "            print('Pairs analyzed: ', n)\n",
        "        result.append(path_similarity_sentence(row[0],row[1], tag))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 468,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([list(['leader', 'give', 'new', 'chance', 'let', 'hope', 'seize']),\n",
              "       list(['leader', 'benefit', 'aujourd', 'hui', 'new', 'luck', 'let', 'let', 'therefore', 'seize'])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 468,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 467,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pairs analyzed:  0\n",
            "Pairs analyzed:  50\n",
            "Pairs analyzed:  100\n",
            "Pairs analyzed:  150\n",
            "Pairs analyzed:  200\n",
            "Pairs analyzed:  250\n",
            "Pairs analyzed:  300\n",
            "Pairs analyzed:  350\n",
            "Pairs analyzed:  400\n",
            "Pairs analyzed:  450\n",
            "Pairs analyzed:  500\n",
            "Pairs analyzed:  550\n",
            "Pairs analyzed:  600\n",
            "Pairs analyzed:  650\n",
            "Pairs analyzed:  700\n",
            "Pairs analyzed:  750\n",
            "Pairs analyzed:  800\n",
            "Pairs analyzed:  850\n",
            "Pairs analyzed:  900\n",
            "Pairs analyzed:  950\n",
            "Pairs analyzed:  1000\n",
            "Pairs analyzed:  1050\n",
            "Pairs analyzed:  1100\n",
            "Pairs analyzed:  1150\n",
            "Pairs analyzed:  1200\n",
            "Pairs analyzed:  1250\n",
            "Pairs analyzed:  1300\n",
            "Pairs analyzed:  1350\n",
            "Pairs analyzed:  1400\n",
            "Pairs analyzed:  1450\n",
            "Pairs analyzed:  1500\n",
            "Pairs analyzed:  1550\n",
            "Pairs analyzed:  1600\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\mario\\Github\\UPC\\IHLT - Introduction to Human Language Technology\\Project_LaurenTucker_MarioRosas.ipynb Celda 73\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([total1\u001b[39m.\u001b[39mlemmatize_data(), total2\u001b[39m.\u001b[39mlemmatize_data()], dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39mT\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m path_similarity(data)\n",
            "\u001b[1;32mc:\\Users\\mario\\Github\\UPC\\IHLT - Introduction to Human Language Technology\\Project_LaurenTucker_MarioRosas.ipynb Celda 73\u001b[0m in \u001b[0;36mpath_similarity\u001b[1;34m(data, tag)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m n\u001b[39m%\u001b[39m\u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPairs analyzed: \u001b[39m\u001b[39m'\u001b[39m, n)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     result\u001b[39m.\u001b[39mappend(path_similarity_sentence(row[\u001b[39m0\u001b[39;49m],row[\u001b[39m1\u001b[39;49m], tag))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "\u001b[1;32mc:\\Users\\mario\\Github\\UPC\\IHLT - Introduction to Human Language Technology\\Project_LaurenTucker_MarioRosas.ipynb Celda 73\u001b[0m in \u001b[0;36mpath_similarity_sentence\u001b[1;34m(lemmas1, lemmas2, tag)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     lemmas1 \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(lemmas1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     lemmas2 \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(lemmas2)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mean1 \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m(\u001b[39mmax\u001b[39;49m([path_similarity_word(l1,l2, tag) \u001b[39mfor\u001b[39;49;00m l2 \u001b[39min\u001b[39;49;00m lemmas2]) \u001b[39mfor\u001b[39;49;00m l1 \u001b[39min\u001b[39;49;00m lemmas1)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(lemmas1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m mean2 \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mmax\u001b[39m([path_similarity_word(l2,l1, tag) \u001b[39mfor\u001b[39;00m l1 \u001b[39min\u001b[39;00m lemmas1]) \u001b[39mfor\u001b[39;00m l2 \u001b[39min\u001b[39;00m lemmas2)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(lemmas2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m mean1 \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m mean2 \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "\u001b[1;32mc:\\Users\\mario\\Github\\UPC\\IHLT - Introduction to Human Language Technology\\Project_LaurenTucker_MarioRosas.ipynb Celda 73\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     lemmas1 \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(lemmas1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     lemmas2 \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(lemmas2)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mean1 \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mmax\u001b[39m([path_similarity_word(l1,l2, tag) \u001b[39mfor\u001b[39;00m l2 \u001b[39min\u001b[39;00m lemmas2]) \u001b[39mfor\u001b[39;00m l1 \u001b[39min\u001b[39;00m lemmas1)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(lemmas1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m mean2 \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mmax\u001b[39m([path_similarity_word(l2,l1, tag) \u001b[39mfor\u001b[39;00m l1 \u001b[39min\u001b[39;00m lemmas1]) \u001b[39mfor\u001b[39;00m l2 \u001b[39min\u001b[39;00m lemmas2)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(lemmas2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m mean1 \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m mean2 \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "\u001b[1;32mc:\\Users\\mario\\Github\\UPC\\IHLT - Introduction to Human Language Technology\\Project_LaurenTucker_MarioRosas.ipynb Celda 73\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     lemmas1 \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(lemmas1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     lemmas2 \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(lemmas2)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mean1 \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mmax\u001b[39m([path_similarity_word(l1,l2, tag) \u001b[39mfor\u001b[39;00m l2 \u001b[39min\u001b[39;00m lemmas2]) \u001b[39mfor\u001b[39;00m l1 \u001b[39min\u001b[39;00m lemmas1)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(lemmas1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m mean2 \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mmax\u001b[39m([path_similarity_word(l2,l1, tag) \u001b[39mfor\u001b[39;00m l1 \u001b[39min\u001b[39;00m lemmas1]) \u001b[39mfor\u001b[39;00m l2 \u001b[39min\u001b[39;00m lemmas2)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(lemmas2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m mean1 \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m mean2 \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "\u001b[1;32mc:\\Users\\mario\\Github\\UPC\\IHLT - Introduction to Human Language Technology\\Project_LaurenTucker_MarioRosas.ipynb Celda 73\u001b[0m in \u001b[0;36mpath_similarity_word\u001b[1;34m(lemma1, lemma2, tag)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m synsets1 \u001b[39m=\u001b[39m get_synsets(lemma1, tag)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m synsets2 \u001b[39m=\u001b[39m get_synsets(lemma2, tag)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m similarities \u001b[39m=\u001b[39m [syn1\u001b[39m.\u001b[39mpath_similarity(syn2) \u001b[39mfor\u001b[39;00m syn1 \u001b[39min\u001b[39;00m synsets2 \u001b[39mfor\u001b[39;00m syn2 \u001b[39min\u001b[39;00m synsets2]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m similarities \u001b[39m!=\u001b[39m []:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(similarities)\n",
            "\u001b[1;32mc:\\Users\\mario\\Github\\UPC\\IHLT - Introduction to Human Language Technology\\Project_LaurenTucker_MarioRosas.ipynb Celda 73\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m synsets1 \u001b[39m=\u001b[39m get_synsets(lemma1, tag)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m synsets2 \u001b[39m=\u001b[39m get_synsets(lemma2, tag)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m similarities \u001b[39m=\u001b[39m [syn1\u001b[39m.\u001b[39;49mpath_similarity(syn2) \u001b[39mfor\u001b[39;00m syn1 \u001b[39min\u001b[39;00m synsets2 \u001b[39mfor\u001b[39;00m syn2 \u001b[39min\u001b[39;00m synsets2]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m similarities \u001b[39m!=\u001b[39m []:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mario/Github/UPC/IHLT%20-%20Introduction%20to%20Human%20Language%20Technology/Project_LaurenTucker_MarioRosas.ipynb#Y224sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(similarities)\n",
            "File \u001b[1;32mc:\\Users\\mario\\anaconda3\\envs\\IHLT\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:866\u001b[0m, in \u001b[0;36mSynset.path_similarity\u001b[1;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpath_similarity\u001b[39m(\u001b[39mself\u001b[39m, other, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, simulate_root\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    838\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[39m    Path Distance Similarity:\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[39m    Return a score denoting how similar two word senses are, based on the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[39m        itself.\u001b[39;00m\n\u001b[0;32m    862\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    864\u001b[0m     distance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortest_path_distance(\n\u001b[0;32m    865\u001b[0m         other,\n\u001b[1;32m--> 866\u001b[0m         simulate_root\u001b[39m=\u001b[39msimulate_root \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_needs_root() \u001b[39mor\u001b[39;00m other\u001b[39m.\u001b[39m_needs_root()),\n\u001b[0;32m    867\u001b[0m     )\n\u001b[0;32m    868\u001b[0m     \u001b[39mif\u001b[39;00m distance \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m distance \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    869\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mario\\anaconda3\\envs\\IHLT\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:474\u001b[0m, in \u001b[0;36mSynset._needs_root\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_needs_root\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 474\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m==\u001b[39m NOUN \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wordnet_corpus_reader\u001b[39m.\u001b[39;49mget_version() \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1.6\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    475\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mario\\anaconda3\\envs\\IHLT\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1380\u001b[0m, in \u001b[0;36mWordNetCorpusReader.get_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1378\u001b[0m fh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_file(ADJ)\n\u001b[0;32m   1379\u001b[0m fh\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[1;32m-> 1380\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m fh:\n\u001b[0;32m   1381\u001b[0m     match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWord[nN]et (\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+) Copyright\u001b[39m\u001b[39m\"\u001b[39m, line)\n\u001b[0;32m   1382\u001b[0m     \u001b[39mif\u001b[39;00m match \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mario\\anaconda3\\envs\\IHLT\\lib\\site-packages\\nltk\\data.py:1152\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext()\n",
            "File \u001b[1;32mc:\\Users\\mario\\anaconda3\\envs\\IHLT\\lib\\site-packages\\nltk\\data.py:1145\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1144\u001b[0m     \u001b[39m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1145\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadline()\n\u001b[0;32m   1146\u001b[0m     \u001b[39mif\u001b[39;00m line:\n\u001b[0;32m   1147\u001b[0m         \u001b[39mreturn\u001b[39;00m line\n",
            "File \u001b[1;32mc:\\Users\\mario\\anaconda3\\envs\\IHLT\\lib\\site-packages\\nltk\\data.py:1109\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.readline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1107\u001b[0m chars \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_chars\n\u001b[0;32m   1108\u001b[0m lines \u001b[39m=\u001b[39m chars\u001b[39m.\u001b[39msplitlines(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 1109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(lines) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1110\u001b[0m     line \u001b[39m=\u001b[39m lines[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinebuffer \u001b[39m=\u001b[39m lines[\u001b[39m1\u001b[39m:]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data = np.array([total1.lemmatize_data(), total2.lemmatize_data()], dtype=object).T\n",
        "path_similarity(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {},
      "outputs": [],
      "source": [
        "similarities = {'wup':{},\n",
        "                'path':{},\n",
        "                'lch':{},\n",
        "                'lin':{}}\n",
        "\n",
        "maxi = {'wup':1, 'path':1, 'lin':1, 'lch':3}\n",
        "\n",
        "v_pos = {'n','v'}\n",
        "\n",
        "def get_synsets(lemma, tag):\n",
        "        if not tag:\n",
        "            synsets = wn.synsets(lemma)\n",
        "        else:\n",
        "            if lemma[1][0] in list(pos_map.keys()):\n",
        "                synsets = wn.synsets(lemma[0], pos_map[lemma[1][0]])\n",
        "            else:\n",
        "                synsets = wn.synsets(lemma[0])\n",
        "        return synsets\n",
        "def similarity_word(method,lemma1,lemma2,tag = False):\n",
        "\n",
        "    if lemma1 == lemma2:\n",
        "        return maxi[method]\n",
        "\n",
        "    if method in similarities:\n",
        "        if (lemma1,lemma2) in similarities[method]:\n",
        "            return similarities[method][(lemma1,lemma2)]\n",
        "\n",
        "    synsets1 = get_synsets(lemma1, tag)\n",
        "    synsets2 = get_synsets(lemma2, tag)\n",
        "    \n",
        "    if method == 'path':\n",
        "        similarities_t = [syn1.path_similarity(syn2) for syn1 in synsets1 for syn2 in synsets2]\n",
        "    elif method == 'wup':\n",
        "        similarities_t = [syn1.wup_similarity(syn2) for syn1 in synsets1 for syn2 in synsets2]\n",
        "    elif method == 'lch':\n",
        "        similarities_t = [syn1.lch_similarity(syn2) for syn1 in synsets1 for syn2 in synsets2 if syn1.pos() == syn2.pos()]\n",
        "    else:\n",
        "        similarities_t = [syn1.lin_similarity(syn2, brown_ic) for syn1 in synsets1 for syn2 in synsets2 if syn1.pos() == syn2.pos() and syn1.pos() in v_pos]\n",
        "\n",
        "    if similarities_t != []:\n",
        "        similarities[method][(lemma1,lemma2)] = max(similarities_t)\n",
        "        return similarities[method][(lemma1,lemma2)]\n",
        "    return 0\n",
        "\n",
        "def similarity_sentence(method,lemmas1, lemmas2, tag = False):\n",
        "    if tag:\n",
        "        lemmas1 = nltk.pos_tag(lemmas1)\n",
        "        lemmas2 = nltk.pos_tag(lemmas2)\n",
        "    mean1 = sum(max([similarity_word(method,l1,l2, tag) for l2 in lemmas2]) for l1 in lemmas1)/len(lemmas1)\n",
        "    mean2 = sum(max([similarity_word(method,l2,l1, tag) for l1 in lemmas1]) for l2 in lemmas2)/len(lemmas2)\n",
        "    if mean1 != 0 and mean2 != 0:\n",
        "        return (2*mean1*mean2)/(mean1+mean2)\n",
        "    return 0\n",
        "\n",
        "def synset_similarity(method, data = False, tag = False):\n",
        "    # j_data = self.data if not data else data\n",
        "    j_data = data\n",
        "    result = []\n",
        "    for n,row in enumerate(j_data):\n",
        "        if n%50 == 0:\n",
        "            print('Pairs analyzed: ', n)\n",
        "        result.append(similarity_sentence(method,row[0],row[1], tag))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to\n",
            "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pairs analyzed:  0\n"
          ]
        }
      ],
      "source": [
        "data = np.array([total1.lemmatize_data(), total2.lemmatize_data()], dtype=object).T\n",
        "res = synset_similarity('path', data[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.7355679702048417,\n",
              " 1.0,\n",
              " 0.5572441742654507,\n",
              " 0.9090909090909091,\n",
              " 1.0,\n",
              " 0.5482758620689654,\n",
              " 0.6253369272237198,\n",
              " 1.0,\n",
              " 0.6802721088435374,\n",
              " 0.9090909090909091]"
            ]
          },
          "execution_count": 313,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3108"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pairs analyzed:  0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.7355679702048417,\n",
              " 1.0,\n",
              " 0.5572441742654507,\n",
              " 0.9090909090909091,\n",
              " 1.0,\n",
              " 0.5482758620689654,\n",
              " 0.6253369272237198,\n",
              " 1.0,\n",
              " 0.6802721088435374,\n",
              " 0.9090909090909091]"
            ]
          },
          "execution_count": 315,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_metrics([total1.lemmatize_data()[:10], total2.lemmatize_data()[:10]], ['synset_similarity'], {'method':'path'}).do()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {},
      "outputs": [],
      "source": [
        "t1,t2 = total1.lemmatized_data, total2.lemmatized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nmalized_length_difference(sentece1, sentece2):\n",
        "    return abs(len(sentece1)-len(sentece2)) / max(len(sentece1), len(sentece2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.3"
            ]
          },
          "execution_count": 319,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "length_difference(t1[0], t2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.3,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.14285714285714285,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.3333333333333333,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.2222222222222222,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.3333333333333333,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.25,\n",
              " 0.3,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.125,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.2222222222222222,\n",
              " 0.3,\n",
              " 0.2857142857142857,\n",
              " 0.3,\n",
              " 0.1111111111111111,\n",
              " 0.2857142857142857,\n",
              " 0.5,\n",
              " 0.0,\n",
              " 0.75,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.16666666666666666,\n",
              " 0.125,\n",
              " 0.3333333333333333,\n",
              " 0.2,\n",
              " 0.14285714285714285,\n",
              " 0.3333333333333333,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.125,\n",
              " 0.125,\n",
              " 0.25,\n",
              " 0.2857142857142857,\n",
              " 0.14285714285714285,\n",
              " 0.3333333333333333,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.3333333333333333,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.14285714285714285,\n",
              " 0.0,\n",
              " 0.2222222222222222,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.125,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.1111111111111111,\n",
              " 0.125,\n",
              " 0.16666666666666666,\n",
              " 0.16666666666666666,\n",
              " 0.16666666666666666,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.16666666666666666,\n",
              " 0.3333333333333333,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.14285714285714285,\n",
              " 0.2857142857142857,\n",
              " 0.3333333333333333,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.2222222222222222,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.125,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.3333333333333333,\n",
              " 0.2222222222222222,\n",
              " 0.2,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.16666666666666666,\n",
              " 0.2857142857142857,\n",
              " 0.14285714285714285,\n",
              " 0.125,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.3333333333333333,\n",
              " 0.3333333333333333,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.2222222222222222,\n",
              " 0.16666666666666666,\n",
              " 0.125,\n",
              " 0.5,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.125,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.3333333333333333,\n",
              " 0.3333333333333333,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.2222222222222222,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.3,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.2,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.125,\n",
              " 0.25,\n",
              " 0.2857142857142857,\n",
              " 0.2857142857142857,\n",
              " 0.16666666666666666,\n",
              " 0.2,\n",
              " 0.3333333333333333,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.4,\n",
              " 0.2857142857142857,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.2,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.25,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2857142857142857,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.3333333333333333,\n",
              " 0.5,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.2,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.3333333333333333,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.4,\n",
              " 0.2,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.3333333333333333,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.4,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.3333333333333333,\n",
              " 0.3333333333333333,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.4,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.2,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.4,\n",
              " 0.2,\n",
              " 0.3333333333333333,\n",
              " 0.4,\n",
              " 0.25,\n",
              " 0.4,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.14285714285714285,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.2857142857142857,\n",
              " 0.25,\n",
              " 0.5,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.5,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.2,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.2,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.16666666666666666,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.3333333333333333,\n",
              " 0.3333333333333333,\n",
              " 0.25,\n",
              " 0.5714285714285714,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.14285714285714285,\n",
              " 0.2,\n",
              " 0.4,\n",
              " 0.4,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.25,\n",
              " 0.25,\n",
              " 0.4,\n",
              " 0.2,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.25,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " ...]"
            ]
          },
          "execution_count": 330,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_metrics([t1,t2], ['norm_lenght_diff']).do()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wup_similarity_word(lemma1,lemma2,tag = False):\n",
        "    synsets1 = get_synsets(lemma1, tag)\n",
        "    synsets2 = get_synsets(lemma2, tag)\n",
        "    similarities = [syn1.path_similarity(syn2) for syn1 in synsets2 for syn2 in synsets2]\n",
        "    if similarities != []:\n",
        "        return max(similarities)\n",
        "    return 0\n",
        "def similarity_sentence(method,lemmas1, lemmas2, tag = False):\n",
        "    if tag:\n",
        "        lemmas1 = nltk.pos_tag(lemmas1)\n",
        "        lemmas2 = nltk.pos_tag(lemmas2)\n",
        "    \n",
        "    mean1 = sum(max([path_similarity_word(l1,l2, tag) for l2 in lemmas2]) for l1 in lemmas1)/len(lemmas1)\n",
        "    mean2 = sum(max([path_similarity_word(l2,l1, tag) for l1 in lemmas1]) for l2 in lemmas2)/len(lemmas2)\n",
        "    if mean1 != 0 and mean2 != 0:\n",
        "        return (2*mean1*mean2)/(mean1+mean2)\n",
        "def similarity(method, data = False, tag = False):\n",
        "    # j_data = self.data if not data else data\n",
        "    j_data = data\n",
        "    result = []\n",
        "    for n,row in enumerate(j_data):\n",
        "        if n%50 == 0:\n",
        "            print('Pairs analyzed: ', n)\n",
        "        result.append(similarity_sentence(method,row[0],row[1], tag))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'give'"
            ]
          },
          "execution_count": 414,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total1.lemmatize_data(r_pos_tag = False)[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Synset('give.n.01'),\n",
              " Synset('give.v.01'),\n",
              " Synset('yield.v.01'),\n",
              " Synset('give.v.03'),\n",
              " Synset('give.v.04'),\n",
              " Synset('give.v.05'),\n",
              " Synset('hold.v.03'),\n",
              " Synset('give.v.07'),\n",
              " Synset('give.v.08'),\n",
              " Synset('give.v.09'),\n",
              " Synset('give.v.10'),\n",
              " Synset('render.v.04'),\n",
              " Synset('impart.v.01'),\n",
              " Synset('establish.v.05'),\n",
              " Synset('give.v.14'),\n",
              " Synset('give.v.15'),\n",
              " Synset('sacrifice.v.01'),\n",
              " Synset('pass.v.05'),\n",
              " Synset('give.v.18'),\n",
              " Synset('give.v.19'),\n",
              " Synset('give.v.20'),\n",
              " Synset('give.v.21'),\n",
              " Synset('grant.v.05'),\n",
              " Synset('move_over.v.01'),\n",
              " Synset('feed.v.02'),\n",
              " Synset('contribute.v.02'),\n",
              " Synset('collapse.v.01'),\n",
              " Synset('give.v.27'),\n",
              " Synset('give.v.28'),\n",
              " Synset('give.v.29'),\n",
              " Synset('afford.v.04'),\n",
              " Synset('give.v.31'),\n",
              " Synset('give.v.32'),\n",
              " Synset('give.v.33'),\n",
              " Synset('give.v.34'),\n",
              " Synset('give.v.35'),\n",
              " Synset('give.v.36'),\n",
              " Synset('give.v.37'),\n",
              " Synset('give.v.38'),\n",
              " Synset('give.v.39'),\n",
              " Synset('give.v.40'),\n",
              " Synset('give.v.41'),\n",
              " Synset('give.v.42'),\n",
              " Synset('give.v.43'),\n",
              " Synset('give.v.44')]"
            ]
          },
          "execution_count": 373,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wn.synsets(total1.lemmatize_data(r_pos_tag = False)[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "t1,t2 = nltk.pos_tag(r1), nltk.pos_tag(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "match = {'JJ':\"a\", 'JJ':\"s\", 'RB':\"r\", 'NN':\"n\", 'VB':\"v\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "s1,s2 = wn.synsets(t1[2][0], match[t1[2][1]]), wn.synsets(t2[2][0], match[t2[2][1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([Synset('new.a.01'),\n",
              "  Synset('fresh.s.04'),\n",
              "  Synset('raw.s.12'),\n",
              "  Synset('new.s.04'),\n",
              "  Synset('new.s.05'),\n",
              "  Synset('new.a.06'),\n",
              "  Synset('newfangled.s.01'),\n",
              "  Synset('new.s.08'),\n",
              "  Synset('modern.s.05'),\n",
              "  Synset('new.s.10'),\n",
              "  Synset('new.s.11')],\n",
              " [Synset('new.a.01'),\n",
              "  Synset('fresh.s.04'),\n",
              "  Synset('raw.s.12'),\n",
              "  Synset('new.s.04'),\n",
              "  Synset('new.s.05'),\n",
              "  Synset('new.a.06'),\n",
              "  Synset('newfangled.s.01'),\n",
              "  Synset('new.s.08'),\n",
              "  Synset('modern.s.05'),\n",
              "  Synset('new.s.10'),\n",
              "  Synset('new.s.11')])"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s1,s2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "l = Counter([j for i in s1 for j in i.lemmas()]).most_common(1)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Lemma('new.a.01.new')]\n",
            "[Lemma('fresh.s.04.fresh'), Lemma('fresh.s.04.new'), Lemma('fresh.s.04.novel')]\n",
            "[Lemma('raw.s.12.raw'), Lemma('raw.s.12.new')]\n",
            "[Lemma('new.s.04.new'), Lemma('new.s.04.unexampled')]\n",
            "[Lemma('new.s.05.new')]\n",
            "[Lemma('new.a.06.new')]\n",
            "[Lemma('newfangled.s.01.newfangled'), Lemma('newfangled.s.01.new')]\n",
            "[Lemma('new.s.08.New')]\n",
            "[Lemma('modern.s.05.Modern'), Lemma('modern.s.05.New')]\n",
            "[Lemma('new.s.10.new'), Lemma('new.s.10.young')]\n",
            "[Lemma('new.s.11.new')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for i in range(len(s1)):\n",
        "    print(s1[i].lemmas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Lemma('newfangled.s.01.new')"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s1[6].lemmas()[1]#.synset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_n(n,p):\n",
        "    if p:\n",
        "        return n\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(print_n(5,False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('leaders', 'leaders')"
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w1,w2 = total1.clean_data()[0][0], total2.clean_data()[0][0]\n",
        "w1,w2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('leaders', 'NNS'),\n",
              " ('given', 'VBN'),\n",
              " ('new', 'JJ'),\n",
              " ('chance', 'NN'),\n",
              " ('let', 'VBD'),\n",
              " ('hope', 'PRP'),\n",
              " ('seize', 'VB')]"
            ]
          },
          "execution_count": 265,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tags1 =nltk.pos_tag(total1.clean_data()[0])\n",
        "tags1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('leaders', 'n')"
            ]
          },
          "execution_count": 270,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tags1[0][0], tags1[0][1][0].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Synset('leadership.n.02'), Synset('leader.n.01'), Synset('drawing_card.n.02')]"
            ]
          },
          "execution_count": 271,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "synsets1 = wn.synsets(tags1[0][0], tags1[0][1][0].lower())\n",
        "synsets1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Lemma('drawing_card.n.02.leader'),\n",
              " Lemma('drawing_card.n.02.loss_leader'),\n",
              " Lemma('drawing_card.n.02.drawing_card'),\n",
              " Lemma('leader.n.01.leader'),\n",
              " Lemma('leadership.n.02.leaders'),\n",
              " Lemma('leadership.n.02.leadership')]"
            ]
          },
          "execution_count": 260,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[lemma for synset in synsets1 for lemma in synset.lemmas()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Synset('leader.n.01')"
            ]
          },
          "execution_count": 259,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter([lemma for synset in synsets1 for lemma in synset.lemmas()]).most_common(1)[0][0].synset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {},
      "outputs": [],
      "source": [
        "wnl = nltk.stem.WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'leader'"
            ]
          },
          "execution_count": 276,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wnl.lemmatize('leaders', 'n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " -- Data hasn't been cleaned, to lemmatize the data is going to be cleaned with the default parameters --\n",
            "\n"
          ]
        }
      ],
      "source": [
        "g = text_processing(europarl[1]).lemmatize_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
        "\n",
        "pos_map = {'N': NOUN,\n",
        "                        'V':VERB,\n",
        "                        'J':ADJ,\n",
        "                        'R':ADV}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_values(['n', 'v', 'a', 'r'])"
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_map.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {},
      "outputs": [],
      "source": [
        "ps = nltk.pos_tag(g[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "leader ---- NN --> [Synset('leader.n.01'), Synset('drawing_card.n.02')]\n",
            "benefit ---- NN --> [Synset('benefit.n.01'), Synset('benefit.n.02'), Synset('benefit.n.03')]\n",
            "aujourd ---- JJ --> []\n",
            "hui ---- VBZ --> []\n",
            "new ---- JJ --> [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11')]\n",
            "luck ---- JJ --> []\n",
            "let ---- NN --> [Synset('lashkar-e-taiba.n.01'), Synset('let.n.02')]\n",
            "let ---- VB --> [Synset('let.v.01'), Synset('let.v.02'), Synset('permit.v.01'), Synset('get.v.03'), Synset('let.v.05'), Synset('lease.v.03')]\n",
            "therefore ---- VB --> []\n",
            "seize ---- VB --> [Synset('seize.v.01'), Synset('seize.v.02'), Synset('appropriate.v.02'), Synset('impound.v.01'), Synset('assume.v.06'), Synset('seize.v.06'), Synset('seize.v.07'), Synset('grab.v.06')]\n"
          ]
        }
      ],
      "source": [
        "for i in ps:\n",
        "    print(i[0],'----',i[1],'-->',wn.synsets(i[0], pos_map[i[1][0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from text_processing import text_processing, compute_metrics\n",
        "from scipy.stats import pearsonr\n",
        "from collections import Counter\n",
        "import io\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# --------------- Training Data\n",
        "europarl_Train = pd.read_csv('Complementary Material/train/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "europarl_Train['gs'] = pd.read_csv('Complementary Material/train/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "\n",
        "vid_Train = pd.read_csv('Complementary Material/train/STS.input.MSRvid.txt',sep='\\t',header=None)\n",
        "vid_Train['gs'] = pd.read_csv('Complementary Material/train/STS.gs.MSRvid.txt',sep='\\t',header=None)\n",
        "\n",
        "with open('Complementary Material/train/STS.input.MSRpar.txt') as f:\n",
        "    lines = f.readlines()\n",
        "for index in range(len(lines)):\n",
        "    lines[index] = lines[index].replace('\\\"', ' ')\n",
        "par_Train = pd.read_csv(io.StringIO(''.join(lines)), sep='\\t',header=None, on_bad_lines='warn')\n",
        "par_Train['gs'] = pd.read_csv('Complementary Material/train/STS.gs.MSRpar.txt',sep='\\t',header=None)\n",
        "\n",
        "total_Train = pd.concat([europarl_Train, vid_Train, par_Train]).reset_index(drop=True)\n",
        "\n",
        "# --------------- Testing Data\n",
        "europarl_Test = pd.read_csv('Complementary Material/test-gold/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "europarl_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "\n",
        "vid_Test = pd.read_csv('Complementary Material/test-gold/STS.input.MSRvid.txt',sep='\\t',header=None)\n",
        "vid_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.MSRvid.txt',sep='\\t',header=None)\n",
        "\n",
        "with open('Complementary Material/test-gold/STS.input.MSRpar.txt', encoding='utf8') as f:\n",
        "    lines = f.readlines()\n",
        "for index in range(len(lines)):\n",
        "    lines[index] = lines[index].replace('\\\"', ' ')\n",
        "par_Test = pd.read_csv(io.StringIO(''.join(lines)), sep='\\t',header=None, on_bad_lines='warn')\n",
        "par_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.MSRpar.txt',sep='\\t',header=None)\n",
        "\n",
        "onwn_Test = pd.read_csv('Complementary Material/test-gold/STS.input.surprise.OnWN.txt',sep='\\t',header=None)\n",
        "onwn_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.surprise.OnWN.txt',sep='\\t',header=None)\n",
        "\n",
        "news_Test = pd.read_csv('Complementary Material/test-gold/STS.input.surprise.SMTnews.txt',sep='\\t',header=None)\n",
        "news_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.surprise.SMTnews.txt',sep='\\t',header=None) \n",
        "\n",
        "total_Test = pd.concat([europarl_Test, vid_Test, par_Test, onwn_Test, news_Test]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "import nltk\n",
        "lowercase = True\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "signs = string.punctuation\n",
        "minwords_len = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "Train1 = text_processing(total_Train[0])\n",
        "Train2 = text_processing(total_Train[1])\n",
        "Train1.clean_data(auto=False, lowercase = lowercase, signs = signs),Train2.clean_data(auto=False, lowercase = lowercase, signs = signs)\n",
        "#Train1.clean_data(),Train2.clean_data()\n",
        "_,_ = Train1.lemmatize_data(), Train2.lemmatize_data() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = 'Complementary Material/Extracted Features/Train/unigram_semi_clean'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = compute_metrics([Train1.cleaned_data,Train2.cleaned_data],['unigram']).do(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.moveaxis(np.array(X),0,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2234, 1)"
            ]
          },
          "execution_count": 357,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.load('Complementary Material/Extracted Features/Train/PathSym_lemmas.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2234, 1)"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = np.moveaxis(np.array(X),0,-1)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "Test1 = text_processing(total_Test[0])\n",
        "Test2 = text_processing(total_Test[1])\n",
        "Test1.clean_data(),Test2.clean_data()\n",
        "_,_ = Test1.lemmatize_data(), Test2.lemmatize_data() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = 'Complementary Material/Extracted Features/Test/trigram_comp_clean'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = compute_metrics([Test1.cleaned_data,Test2.cleaned_data],['trigram']).do(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = np.moveaxis(np.array(X_test),0,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3108, 1)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test = np.load('Complementary Material/Extracted Features/Test/PathSym_lemmas.npy')\n",
        "X_test = np.moveaxis(np.array(X_test),0,-1)\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature = 'trigram'\n",
        "\n",
        "Train1 = text_processing(total_Train[0])\n",
        "Train2 = text_processing(total_Train[1])\n",
        "#Train1.clean_data(auto=False, lowercase = lowercase, signs = signs),Train2.clean_data(auto=False, lowercase = lowercase, signs = signs)\n",
        "Train1.clean_data(),Train2.clean_data()\n",
        "_,_ = Train1.lemmatize_data(), Train2.lemmatize_data() \n",
        "\n",
        "save_train_path = 'Complementary Material/Extracted Features/Train/'+feature+'_comp_clean'\n",
        "X = compute_metrics([Train1.cleaned_data,Train2.cleaned_data],[feature]).do(save_train_path)\n",
        "\n",
        "Test1 = text_processing(total_Test[0])\n",
        "Test2 = text_processing(total_Test[1])\n",
        "#Test1.clean_data(auto=False, lowercase = lowercase, signs = signs),Test2.clean_data(auto=False, lowercase = lowercase, signs = signs)\n",
        "Test1.clean_data(),Test2.clean_data()\n",
        "_,_ = Test1.lemmatize_data(), Test2.lemmatize_data() \n",
        "\n",
        "save_test_path = 'Complementary Material/Extracted Features/Test/'+feature+'_comp_clean'\n",
        "X_test = compute_metrics([Test1.cleaned_data,Test2.cleaned_data],[feature]).do(save_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classifiers Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=0.7406540348318249, pvalue=0.0)"
            ]
          },
          "execution_count": 251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'bigram_semi_clean', 'trigram_semi_clean', 'jaccard_comp_clean', 'unigram_comp_clean', 'bigram_comp_clean']\n",
        "train_feats = np.load('Complementary Material/Extracted Features/Train/'+features_to_try[0]+'.npy')\n",
        "for n in range(1, len(features_to_try)):\n",
        "    train_feats = np.concatenate((train_feats, np.load('Complementary Material/Extracted Features/Train/'+features_to_try[n]+'.npy')))\n",
        "train_feats = np.moveaxis(np.array(train_feats),0,-1)\n",
        "\n",
        "y = total_Train['gs'].values\n",
        "\n",
        "regr = make_pipeline(StandardScaler(), SVR(kernel = 'rbf', C=1, epsilon=0, gamma = 'scale', tol=1e-1))\n",
        "regr.fit(train_feats, y)\n",
        "\n",
        "test_feats = np.load('Complementary Material/Extracted Features/Test/'+features_to_try[0]+'.npy')\n",
        "for n in range(1, len(features_to_try)):\n",
        "    test_feats = np.concatenate((test_feats, np.load('Complementary Material/Extracted Features/Test/'+features_to_try[n]+'.npy')))\n",
        "test_feats = np.moveaxis(np.array(test_feats),0,-1)\n",
        "\n",
        "pearsonr(total_Test['gs'].values,regr.predict(test_feats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_hastie_10_2\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=0.7383876997011208, pvalue=0.0)"
            ]
          },
          "execution_count": 252,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'bigram_semi_clean', 'trigram_semi_clean', 'jaccard_comp_clean', 'unigram_comp_clean', 'bigram_comp_clean']\n",
        "#features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'jaccard_comp_clean', 'unigram_comp_clean']\n",
        "train_feats = np.load('Complementary Material/Extracted Features/Train/'+features_to_try[0]+'.npy')\n",
        "for n in range(1, len(features_to_try)):\n",
        "    train_feats = np.concatenate((train_feats, np.load('Complementary Material/Extracted Features/Train/'+features_to_try[n]+'.npy')))\n",
        "train_feats = np.moveaxis(np.array(train_feats),0,-1)\n",
        "\n",
        "y = total_Train['gs'].values\n",
        "\n",
        "gbr = GradientBoostingRegressor(random_state=17)\n",
        "gbr.fit(train_feats, y)\n",
        "\n",
        "\n",
        "test_feats = np.load('Complementary Material/Extracted Features/Test/'+features_to_try[0]+'.npy')\n",
        "for n in range(1, len(features_to_try)):\n",
        "    test_feats = np.concatenate((test_feats, np.load('Complementary Material/Extracted Features/Test/'+features_to_try[n]+'.npy')))\n",
        "test_feats = np.moveaxis(np.array(test_feats),0,-1)\n",
        "\n",
        "pearsonr(total_Test['gs'].values,gbr.predict(test_feats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=0.7518254148216057, pvalue=0.0)"
            ]
          },
          "execution_count": 331,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'bigram_semi_clean', 'trigram_semi_clean', 'jaccard_comp_clean', 'unigram_comp_clean', 'bigram_comp_clean']\n",
        "#features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'bigram_semi_clean', 'trigram_semi_clean']\n",
        "train_feats = np.load('Complementary Material/Extracted Features/Train/'+features_to_try[0]+'.npy', allow_pickle=True)\n",
        "for n in range(1, len(features_to_try)):\n",
        "    train_feats = np.concatenate((train_feats, np.load('Complementary Material/Extracted Features/Train/'+features_to_try[n]+'.npy')))\n",
        "train_feats = np.moveaxis(np.array(train_feats),0,-1)\n",
        "\n",
        "y = total_Train['gs'].values\n",
        "\n",
        "#mlp = MLPRegressor(hidden_layer_sizes=(100,500,1000,500,100), random_state=5, max_iter=900)\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(100,100,100, 100), random_state=3, max_iter=200)\n",
        "mlp.fit(train_feats, y)\n",
        "\n",
        "test_feats = np.load('Complementary Material/Extracted Features/Test/'+features_to_try[0]+'.npy', allow_pickle=True)\n",
        "for n in range(1, len(features_to_try)):\n",
        "    test_feats = np.concatenate((test_feats, np.load('Complementary Material/Extracted Features/Test/'+features_to_try[n]+'.npy')))\n",
        "test_feats = np.moveaxis(np.array(test_feats),0,-1)\n",
        "\n",
        "pearsonr(total_Test['gs'].values,mlp.predict(test_feats))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('IHLT')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0795eca24a98e58b2dcbec80c9554a91f94c5c7d4e675f06c8c2f85c434623a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
