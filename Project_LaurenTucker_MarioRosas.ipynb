{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mario-rot/Introduction-to-Human-Language-Technology/blob/main/Project_LaurenTucker_MarioRosas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEp-jhafvsx"
      },
      "source": [
        "# Final Project - ILTH\n",
        "\n",
        "**Students:** Lauren Tucker & Mario Rosas !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0LXucmalIYk"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UG3R_lKSsqa"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zTBIq_k_3Yb",
        "outputId": "21414362-13fe-4e6a-8cc4-c4857505aa25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Introduction-to-Human-Language-Technology'...\n",
            "remote: Enumerating objects: 864, done.\u001b[K\n",
            "remote: Counting objects: 100% (864/864), done.\u001b[K\n",
            "remote: Compressing objects: 100% (792/792), done.\u001b[K\n",
            "remote: Total 864 (delta 117), reused 788 (delta 67), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (864/864), 2.27 MiB | 14.46 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting svgling\n",
            "  Downloading svgling-0.3.1-py3-none-any.whl (21 kB)\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.3.1 svgwrite-1.4.3\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "git clone https://github.com/mario-rot/Introduction-to-Human-Language-Technology.git\n",
        "cd 'Introduction-to-Human-Language-Technology'\n",
        "mv 'Complementary Material' /content/\n",
        "\n",
        "pip install svgling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi9UgnrHx1NN"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries to use\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "import collections\n",
        "from collections import Counter\n",
        "import io\n",
        "import string\n",
        "from typing import List, Dict, Tuple\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.metrics import jaccard_distance\n",
        "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "from text_processing import text_processing, compute_metrics\n",
        "\n",
        "dt = pd.read_csv('Complementary Material/test-gold/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "dt['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWRO7TkiSsqb"
      },
      "source": [
        "# Main classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H6-tQ8mSsqc"
      },
      "source": [
        "## text_processing\n",
        "This is a class where we coded different methods to get different preprocessing techniques applied to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghtvVYlnSsqc"
      },
      "outputs": [],
      "source": [
        "class text_processing(): \n",
        "    def __init__(self,\n",
        "             data: List\n",
        "             ):\n",
        "        self.data = data\n",
        "        self.cleaned_data = []\n",
        "        self.tokenized_data = []\n",
        "        self.lemmatized_data = []\n",
        "        self.most_common_lemmatized_data = []\n",
        "        self.lesk_data = []\n",
        "        self.lesk_lemmatized_data = []\n",
        "        self.name_entities_nltk_data = []\n",
        "        self.name_entities_spacy_data = []\n",
        "        self.pos_map = {'N': NOUN,\n",
        "                        'V':VERB,\n",
        "                        'J':ADJ,\n",
        "                        'R':ADV}\n",
        "        self.match = {'j':\"s\", 'j':\"a\", 'r':\"r\", 'n':\"n\", 'v':\"v\"}\n",
        "        self.correcting = {'n':'n', 'v':'v', 'j':'a', 'r':'r'}\n",
        "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def clean_data(self, data = False, auto = True, lowercase = False, stopwords = False, minwords_len = False, signs = False):\n",
        "        self.cleaned_data = []\n",
        "        c_data = self.data if not data else data\n",
        "        if auto:\n",
        "            lowercase = True\n",
        "            stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "            signs = string.punctuation\n",
        "            minwords_len = 2\n",
        "            for element in c_data:\n",
        "                self.cleaned_data.append(self.clean_sentence(element, lowercase, stopwords, minwords_len, signs))\n",
        "        else: \n",
        "            for element in c_data:\n",
        "                 self.cleaned_data.append(self.clean_sentence(element, lowercase, stopwords, minwords_len, signs))\n",
        "        return self.cleaned_data\n",
        "\n",
        "    def clean_sentence(self,sentence, lowercase = True, stopwords = False, minwords_len = False, signs = False):\n",
        "        sentence = sentence.split(' ')\n",
        "        if lowercase:\n",
        "            sentence = [word.lower() for word in sentence]\n",
        "        if signs:\n",
        "            sentence = [word if not any(caracter in signs for caracter in word) else self.remove_signs(word, signs) for word in sentence]\n",
        "        if stopwords:\n",
        "            sentence = [word for word in sentence if word not in stopwords and word.isalpha()]\n",
        "        if minwords_len:\n",
        "            sentence = [word for word in sentence if len(word) > minwords_len]\n",
        "        return sentence\n",
        "\n",
        "    def tokenize_data(self, data = False):\n",
        "        self.tokenized_data = []\n",
        "        t_data = self.data if not data else data\n",
        "        for element in t_data:\n",
        "            self.tokenized_data.append(nltk.word_tokenize(element))\n",
        "        return self.tokenized_data\n",
        "\n",
        "    def frequency(self, Global = False, type_data = 'cleaned'):\n",
        "        if type_data == 'cleaned':\n",
        "            if self.cleaned_data == []:\n",
        "                print('\\n -- Data hasn\\'t been cleaned, to calculate frequency, the data is going to be cleaned with the default parameters --\\n')\n",
        "                self.clean_data()\n",
        "            f_data = self.cleaned_data\n",
        "        elif type_data == 'tokenized':\n",
        "            if self.tokenized_data == []:\n",
        "                print('\\n -- Data hasn\\'t been tokenized, to calculate frequency, the data is going to be tokenized --\\n')\n",
        "                self.tokenize_data()\n",
        "            f_data = self.tokenized_data\n",
        "        if not Global:\n",
        "            frequency = []\n",
        "            for element in f_data:\n",
        "                frequency.append(pd.Series({k:v for k,v in collections.Counter(element).most_common()}))\n",
        "            return frequency\n",
        "        else: \n",
        "            t_data = [element for sublist in f_data for element in sublist]\n",
        "            frequency = pd.Series({k:v for k,v in collections.Counter(t_data).most_common()})\n",
        "            return frequency\n",
        "\n",
        "    def lemmatize_data(self, type_data = 'cleaned', data = False, r_pos_tag = False):\n",
        "        self.lemmatized_data = []\n",
        "        if data:\n",
        "            l_data = data\n",
        "        elif type_data == 'cleaned':\n",
        "            if self.cleaned_data == []:\n",
        "                print('\\n -- Data hasn\\'t been cleaned, to lemmatize the data is going to be cleaned with the default parameters --\\n')\n",
        "                self.clean_data()\n",
        "            l_data = self.cleaned_data\n",
        "        elif type_data == 'tokenized':\n",
        "            if self.tokenized_data == []:\n",
        "                print('\\n -- Data hasn\\'t been tokenized, to lemmatize the data is going to be tokenized --\\n')\n",
        "                self.tokenize_data()\n",
        "            l_data = self.tokenized_data\n",
        "        for element in l_data:\n",
        "            self.lemmatized_data.append(self.lemmatize_sentence(element, r_pos_tag=r_pos_tag))\n",
        "        return self.lemmatized_data\n",
        "\n",
        "    def lemmatize_sentence(self,sentence, cleaned = True, r_pos_tag = False):\n",
        "        if not cleaned:\n",
        "            sentence = self.clean_data([sentence])[0]\n",
        "        tagged = nltk.pos_tag(sentence)\n",
        "        if r_pos_tag:\n",
        "            return [[self.lemmatize(self, pair), pair[1]] for pair in tagged]\n",
        "        return[self.lemmatize(self, pair) for pair in tagged]\n",
        "\n",
        "    def mc_lemmatize_data(self, type_data = 'cleaned', data = False, lemma = False):\n",
        "        self.most_common_lemmatized_data = []\n",
        "        if data:\n",
        "            l_data = data\n",
        "        elif type_data == 'cleaned':\n",
        "            if self.cleaned_data == []:\n",
        "                print('\\n -- Data hasn\\'t been cleaned, to apply most_common_lemma to data is going to be cleaned with the default parameters --\\n')\n",
        "                self.clean_data()\n",
        "            l_data = self.cleaned_data\n",
        "        elif type_data == 'tokenized':\n",
        "            if self.tokenized_data == []:\n",
        "                print('\\n -- Data hasn\\'t been tokenized, to apply most_common_lemma to data is going to be tokenized --\\n')\n",
        "                self.tokenize_data()\n",
        "            l_data = self.tokenized_data\n",
        "        for element in l_data:\n",
        "            self.most_common_lemmatized_data.append(self.most_common_lemma_sentece(element, lemma = lemma))\n",
        "        return self.most_common_lemmatized_data\n",
        "    \n",
        "    def most_common_lemma_sentece(self,sentence, cleaned = True, lemma = False):\n",
        "        if not cleaned:\n",
        "            sentence = self.clean_data([sentence])[0]\n",
        "        return[self.get_most_common_lemma(self, word, lemma) for word in nltk.pos_tag(sentence)]\n",
        "\n",
        "    def apply_lesk_data(self, type_data = 'cleaned', all = False, data = False):\n",
        "        self.lesk_data = []\n",
        "        if data:\n",
        "            ls_data = data\n",
        "        elif type_data == 'cleaned':\n",
        "            if self.cleaned_data == []:\n",
        "                print('\\n -- Data hasn\\'t been cleaned, to apply lest to data is going to be cleaned with the default parameters --\\n')\n",
        "                self.clean_data()\n",
        "            ls_data = self.cleaned_data\n",
        "        elif type_data == 'tokenized':\n",
        "            if self.tokenized_data == []:\n",
        "                print('\\n -- Data hasn\\'t been tokenized, to apply lest to data is going to be tokenized --\\n')\n",
        "                self.tokenize_data()\n",
        "            ls_data = self.tokenized_data\n",
        "        for element in ls_data:\n",
        "            self.lesk_data.append(self.apply_lesk_sentence(element, all))\n",
        "        return self.lesk_data\n",
        "\n",
        "    def apply_lesk_sentence(self, sentence, all = False):\n",
        "        pairs = nltk.pos_tag(sentence)\n",
        "        synsets = []\n",
        "        for pair in pairs:\n",
        "            word, pos = self.filter_pos(self, pair)\n",
        "            if pos:\n",
        "                synset = nltk.wsd.lesk(sentence, word, pos)\n",
        "            else:\n",
        "                synset = False\n",
        "            if synset:\n",
        "                synsets.append(synset.lemmas()[0].name())\n",
        "            elif not synset and all:\n",
        "                synsets.append(pair[0])\n",
        "        return synsets\n",
        "\n",
        "    def apply_lesk_lemmas_data(self, type_data = 'cleaned', all = False, data = False):\n",
        "        self.lesk_lemmatized_data = []\n",
        "        if data:\n",
        "            ls_data = data\n",
        "        elif type_data == 'cleaned':\n",
        "            if self.cleaned_data == []:\n",
        "                print('\\n -- Data hasn\\'t been cleaned, to lesk lemmatize the data is going to be cleaned with the default parameters --\\n')\n",
        "                self.clean_data()\n",
        "            ls_data = self.cleaned_data\n",
        "        elif type_data == 'tokenized':\n",
        "            if self.tokenized_data == []:\n",
        "                print('\\n -- Data hasn\\'t been tokenized, to lesk lemmatize the data is going to be tokenized --\\n')\n",
        "                self.tokenize_data()\n",
        "            ls_data = self.tokenized_data\n",
        "        for element in ls_data:\n",
        "            self.lesk_lemmatized_data.append(self.apply_lesk_lemmas_sentence(element, all))\n",
        "        return self.lesk_lemmatized_data\n",
        "\n",
        "    def apply_lesk_lemmas_sentence(self, sentence, all = False):\n",
        "        lemmatized_sentence = self.lemmatize_sentence(sentence, r_pos_tag = True)\n",
        "        synsets = []\n",
        "        for pair in lemmatized_sentence:\n",
        "            word, pos = self.filter_pos(self, pair)\n",
        "            if pos:\n",
        "                synset = nltk.wsd.lesk(sentence, word, pos)\n",
        "            else:\n",
        "                synset = False\n",
        "            if synset:\n",
        "                synsets.append(synset.lemmas()[0].name())\n",
        "            elif not synset and all:\n",
        "                synsets.append(pair[0])\n",
        "        return synsets\n",
        "\n",
        "    def name_entities_nltk(self, type_data = 'cleaned', data = False):\n",
        "        self.name_entities_nltk_data = []\n",
        "        if data:\n",
        "            ls_data = data\n",
        "        elif type_data == 'cleaned':\n",
        "            if self.cleaned_data == []:\n",
        "                print('\\n -- Data hasn\\'t been cleaned, to get named entitites the data is going to be cleaned with the default parameters --\\n')\n",
        "                self.clean_data()\n",
        "            ls_data = self.cleaned_data\n",
        "        elif type_data == 'tokenized':\n",
        "            if self.tokenized_data == []:\n",
        "                print('\\n -- Data hasn\\'t been tokenized, to get named entitites the data is going to be tokenized --\\n')\n",
        "                self.tokenize_data()\n",
        "            ls_data = self.tokenized_data\n",
        "        for element in ls_data:\n",
        "            self.name_entities_nltk_data.append(self.named_entities_nltk_sentence(element))\n",
        "        return self.name_entities_nltk_data \n",
        "\n",
        "    def named_entities_nltk_sentence(self, sentence):\n",
        "        x = nltk.pos_tag(sentence)\n",
        "        res = nltk.ne_chunk(x)\n",
        "        named_entities = []\n",
        "        for item in res:\n",
        "            try: \n",
        "                ne = item.label()\n",
        "                named_entities.append(ne)\n",
        "            except:\n",
        "                named_entities.append(item[0])\n",
        "\n",
        "        if named_entities != []:\n",
        "            return named_entities\n",
        "\n",
        "    def name_entities_spacy(self):\n",
        "        ne_data = self.data\n",
        "        for element in ne_data:\n",
        "            self.name_entities_spacy_data.append(self.name_entities_spacy_sentence(element))\n",
        "        return self.name_entities_spacy_data\n",
        "\n",
        "    def name_entities_spacy_sentence(self, sentence):\n",
        "        doc = nlp(sentence)\n",
        "        with doc.retokenize() as retokenizer:\n",
        "            tokens = [token for token in doc]\n",
        "            for ent in doc.ents:\n",
        "                retokenizer.merge(doc[ent.start:ent.end], \n",
        "                                attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n",
        "        res = []\n",
        "        for ent in doc:\n",
        "            if ent.ent_type_ != '':\n",
        "                res.append(ent.ent_type_)\n",
        "                # pass\n",
        "            else:\n",
        "                res.append(ent.text) \n",
        "            # res.append(ent.text)\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_signs(wrd,signs):\n",
        "        wrd = list(wrd)\n",
        "        wrd = [word for word in wrd if not any(caracter in signs for caracter in word)]\n",
        "        wrd = ''.join(wrd)\n",
        "        return wrd\n",
        "\n",
        "    @staticmethod\n",
        "    def lemmatize(self, p):\n",
        "        if p[1][0] in {'N','V','J','R'}:\n",
        "            return self.wnl.lemmatize(p[0].lower(), pos=self.pos_map[p[1][0]])\n",
        "        return p[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_most_common_lemma(self,pair, lemma = False):\n",
        "        try:\n",
        "            synsets = wn.synsets(pair[0], self.match[pair[1][0].lower()])\n",
        "            if synsets != []:\n",
        "                if lemma:\n",
        "                    return Counter([j for i in synsets for j in i.lemmas()]).most_common(1)[0][0]\n",
        "                return Counter([j for i in synsets for j in i.lemmas()]).most_common(1)[0][0].name()\n",
        "            else:\n",
        "                if lemma:\n",
        "                    return Counter([j for i in wn.synsets(pair[0]) for j in i.lemmas()]).most_common(1)[0][0]\n",
        "                return Counter([j for i in wn.synsets(pair[0]) for j in i.lemmas()]).most_common(1)[0][0].name()\n",
        "        except:\n",
        "            if not lemma:\n",
        "                return pair[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def filter_pos(self, pair):\n",
        "        if pair[1][0].lower() in list(self.correcting.keys()):\n",
        "            return pair[0], self.correcting[pair[1][0].lower()]\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDcWSlTCSsqd"
      },
      "source": [
        "## Compute metrics\n",
        "\n",
        "Within this class we codded different similarity techniques tu compare the preprocessed data. Then, we will use these computations of different similarity measures to fit different models in order to predict over the test data a proper score of similarity between each pair of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiQARoTKSsqe"
      },
      "outputs": [],
      "source": [
        "class compute_metrics():\n",
        "    def __init__(self,\n",
        "             data: List = False,\n",
        "             metrics: List = False,\n",
        "             kargs: Dict = False,\n",
        "             verbose: bool = False\n",
        "             ):\n",
        "        self.data = np.array(data, dtype = object).T\n",
        "        self.metrics = metrics\n",
        "        self.kargs = kargs if kargs else {}\n",
        "        self.verbose = verbose\n",
        "        self.methods = {'jaccard': self.jaccard,\n",
        "                        'synset_similarity':self.synset_similarity,\n",
        "                        'norm_length_diff': self.normalized_length_difference, \n",
        "                        'cosine': self.cosine_similarity, \n",
        "                        'unigram': self.unigram_similarity,\n",
        "                        'bigram': self.bigram_similarity,\n",
        "                        'trigram': self.trigram_similarity}\n",
        "        self.pos_map = {'N': NOUN,\n",
        "                        'V':VERB,\n",
        "                        'J':ADJ,\n",
        "                        'R':ADV}\n",
        "        self.v_pos = {'n', 'v'}\n",
        "        self.maxi = {'wup':1, 'path':1, 'lin':1, 'lch':3}\n",
        "        self.similarities = {'wup':{},\n",
        "                             'path':{},\n",
        "                             'lch':{},\n",
        "                             'lin':{}}\n",
        "        \n",
        "    def do(self, save = False):\n",
        "        results = []\n",
        "        for num,met in enumerate(self.metrics):\n",
        "            if isinstance(self.kargs, list):\n",
        "                results.append(self.methods[met](**self.kargs[num]))\n",
        "            else:\n",
        "                results.append(self.methods[met](**self.kargs))\n",
        "        if save:\n",
        "            np.save(save, results)\n",
        "        return np.array(results)\n",
        "    \n",
        "    def jaccard(self,data=False):\n",
        "        j_data = self.data if not data else data\n",
        "        result = []\n",
        "        for row in j_data:\n",
        "            result.append(jaccard_distance(set(row[0]),\n",
        "                                           set(row[1]))*10)\n",
        "        return result\n",
        "\n",
        "    def normalized_length_difference(self, data = False):\n",
        "        j_data = self.data if not data else data\n",
        "        result = []\n",
        "        for n,row in enumerate(j_data):\n",
        "            result.append(self.normalized_length_difference_sentece(row[0],row[1]))\n",
        "        return result\n",
        "\n",
        "    def normalized_length_difference_sentece(self,sentece1, sentece2):\n",
        "        return abs(len(sentece1)-len(sentece2)) / max(len(sentece1), len(sentece2))\n",
        "\n",
        "    def synset_similarity(self,method, data = False, tag = False):\n",
        "        j_data = self.data if not data else data\n",
        "        result = []\n",
        "        for n,row in enumerate(j_data):\n",
        "            if self.verbose:\n",
        "                if n%50 == 0:\n",
        "                    print('Pairs analyzed: ', n)\n",
        "            result.append(self.similarity_sentence(method,row[0],row[1], tag))\n",
        "        return result\n",
        "\n",
        "    def similarity_sentence(self, method,lemmas1, lemmas2, tag = False):\n",
        "        if tag:\n",
        "            lemmas1 = nltk.pos_tag(lemmas1)\n",
        "            lemmas2 = nltk.pos_tag(lemmas2)\n",
        "        mean1 = sum(max([self.similarity_word(method,l1,l2, tag) for l2 in lemmas2]) for l1 in lemmas1)/len(lemmas1)\n",
        "        mean2 = sum(max([self.similarity_word(method,l2,l1, tag) for l1 in lemmas1]) for l2 in lemmas2)/len(lemmas2)\n",
        "        if mean1 != 0 and mean2 != 0:\n",
        "            return (2*mean1*mean2)/(mean1+mean2)\n",
        "        return 0\n",
        "        \n",
        "    def similarity_word(self,method,lemma1,lemma2,tag = False):\n",
        "\n",
        "        if lemma1 == lemma2:\n",
        "            return self.maxi[method]\n",
        "\n",
        "        if method in self.similarities:\n",
        "            if (lemma1,lemma2) in self.similarities[method]:\n",
        "                return self.similarities[method][(lemma1,lemma2)]\n",
        "\n",
        "        synsets1 = self.get_synsets(lemma1, tag)\n",
        "        synsets2 = self.get_synsets(lemma2, tag)\n",
        "        \n",
        "        if method == 'path':\n",
        "            similarities_t = [syn1.path_similarity(syn2) for syn1 in synsets1 for syn2 in synsets2]\n",
        "        elif method == 'wup':\n",
        "            similarities_t = [syn1.wup_similarity(syn2) for syn1 in synsets1 for syn2 in synsets2]\n",
        "        elif method == 'lch':\n",
        "            similarities_t = [syn1.lch_similarity(syn2) if syn1.pos() == syn2.pos() else 0 for syn1 in synsets1 for syn2 in synsets2]\n",
        "        elif method == 'lin':\n",
        "            similarities_t = [syn1.lin_similarity(syn2, brown_ic) if syn1.pos() == syn2.pos() and syn2.pos() in self.v_pos else 0 for syn1 in synsets1 for syn2 in synsets2]\n",
        "            \n",
        "        if similarities_t != []:\n",
        "            self.similarities[method][(lemma1,lemma2)] = max(similarities_t)\n",
        "            return self.similarities[method][(lemma1,lemma2)]\n",
        "        return 0\n",
        "\n",
        "    def cosine_similarity(self, data = False):\n",
        "        j_data = self.data if not data else data\n",
        "        result = []\n",
        "        for row in j_data:\n",
        "            result.append(self.cosine_similarity_sentence(row[0],row[1]))\n",
        "        return result\n",
        "\n",
        "    def cosine_similarity_sentence(self,sentence1, sentence2):\n",
        "        sim = 0\n",
        "        all_words = set(sentence1).union(set(sentence2))\n",
        "        s1vec = []\n",
        "        s2vec = []\n",
        "        for word in all_words:\n",
        "            if word in sentence1:\n",
        "                s1vec.append(1)\n",
        "            else:\n",
        "                s1vec.append(0)\n",
        "            if word in sentence2:\n",
        "                s2vec.append(1)\n",
        "            else:\n",
        "                s2vec.append(0)\n",
        "        sim = np.dot(np.array(s1vec), np.array(s2vec)) / (math.sqrt(sum(s1vec)*sum(s2vec)))\n",
        "        return sim\n",
        "\n",
        "    def unigram_similarity(self, data = False, remove_duplicates = False):\n",
        "        j_data = self.data if not data else data\n",
        "        result = []\n",
        "        for row in j_data:\n",
        "            result.append(self.unigram_similarity_sentence(row[0],row[1], remove_duplicates))\n",
        "        return result\n",
        "\n",
        "    def unigram_similarity_sentence(self, sentence1, sentence2, remove_duplicates = False):\n",
        "        if remove_duplicates:\n",
        "            sentence1 = list(set(sentence1))\n",
        "            sentence2 = list(set(sentence2))\n",
        "        total_words = len(sentence1) + len(sentence2)\n",
        "        used_words = []\n",
        "        count = 0\n",
        "        for word in sentence1:\n",
        "            if word in sentence2 and word not in used_words:\n",
        "                count += sentence1.count(word)\n",
        "                count += sentence2.count(word)\n",
        "                used_words.append(word)\n",
        "\n",
        "        sim = count / total_words\n",
        "        return sim\n",
        "\n",
        "    def bigram_similarity(self, data = False, remove_duplicates = False):\n",
        "        j_data = self.data if not data else data\n",
        "        result = []\n",
        "        for row in j_data:\n",
        "            result.append(self.bigram_similarity_sentence(row[0],row[1], remove_duplicates))\n",
        "        return result\n",
        "\n",
        "    def bigram_similarity_sentence(self, sentence1, sentence2, remove_duplicates = False):\n",
        "        bigrams1 = list(nltk.bigrams(sentence1))\n",
        "        bigrams2 = list(nltk.bigrams(sentence2))\n",
        "        if remove_duplicates:\n",
        "            bigrams1 = list(set(bigrams1))\n",
        "            bigrams2 = list(set(bigrams2))\n",
        "        total_bigrams = len(bigrams1) + len(bigrams2)\n",
        "        used_bigrams = []\n",
        "        count = 0\n",
        "        for bigram in bigrams1:\n",
        "            if bigram in bigrams2 and bigram not in used_bigrams:\n",
        "                count += bigrams1.count(bigram)\n",
        "                count += bigrams2.count(bigram)\n",
        "            used_bigrams.append(bigram)\n",
        "        if total_bigrams == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            sim = count / total_bigrams\n",
        "            return sim\n",
        "\n",
        "    def trigram_similarity(self, data = False, remove_duplicates = False):\n",
        "        j_data = self.data if not data else data\n",
        "        result = []\n",
        "        for row in j_data:\n",
        "            result.append(self.trigram_similarity_sentence(row[0],row[1], remove_duplicates))\n",
        "        return result\n",
        "\n",
        "    def trigram_similarity_sentence(self, sentence1, sentence2, remove_duplicates = False):\n",
        "        trigrams1 = list(nltk.trigrams(sentence1))\n",
        "        trigrams2 = list(nltk.trigrams(sentence2))\n",
        "        if remove_duplicates:\n",
        "            trigrams1 = list(set(trigrams1))\n",
        "            trigrams2 = list(set(trigrams2))\n",
        "        total_trigrams = len(trigrams1) + len(trigrams2)\n",
        "        used_trigrams = []\n",
        "        count = 0\n",
        "        for trigram in trigrams1:\n",
        "            if trigram in trigrams2 and trigram not in used_trigrams:\n",
        "                count += trigrams1.count(trigram)\n",
        "                count += trigrams2.count(trigram)\n",
        "            used_trigrams.append(trigram)\n",
        "        if total_trigrams == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            sim = count / total_trigrams\n",
        "            return sim\n",
        "    \n",
        "    def get_synsets(self,lemma,tag):\n",
        "        if not tag:\n",
        "            synsets = wn.synsets(lemma)\n",
        "        else:\n",
        "            if lemma[1][0] in list(self.pos_map.keys()):\n",
        "                synsets = wn.synsets(lemma[0], self.pos_map[lemma[1][0]])\n",
        "            else:\n",
        "                synsets = wn.synsets(lemma[0])\n",
        "        return synsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0FiM7GPSsqe"
      },
      "source": [
        "# train_tags\n",
        "\n",
        "This is a class to easily train different models as word taggers and get the results with an informative table and a connected scatter chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6U0dPCBSsqf"
      },
      "outputs": [],
      "source": [
        "class train_tags():\n",
        "    def __init__(self,\n",
        "             data,\n",
        "             amounts_data,\n",
        "             test_n,\n",
        "             models\n",
        "             ):\n",
        "        self.data = data\n",
        "        self.amount_data = amounts_data\n",
        "        self.test_n = test_n\n",
        "        self.models = models\n",
        "        self.times = {key:[] for key in self.models}\n",
        "        self.total_results = {key:[] for key in self.models}\n",
        "        \n",
        "    def do(self):\n",
        "        pbar = tqdm(total=100)\n",
        "        test_data = self.data[self.test_n:]\n",
        "\n",
        "        for i in tqdm(self.amount_data):\n",
        "            train_data = self.data[:i]\n",
        "\n",
        "            # Hidden Markov Model\n",
        "            if 'HMM' in self.models:\n",
        "                time_before = time.time()\n",
        "                trainer = nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
        "                HMM = trainer.train_supervised(train_data)\n",
        "                self.total_results['HMM'].append(round(HMM.accuracy(test_data), 3))\n",
        "                self.times['HMM'].append(time.time() - time_before)\n",
        "            \n",
        "            # Trigrams'n'Tags\n",
        "            if 'TnT' in self.models:\n",
        "                time_before = time.time()\n",
        "                TnT = nltk.tag.tnt.TnT()\n",
        "                TnT.train(train_data)\n",
        "                self.total_results['TnT'].append(round(TnT.accuracy(test_data), 3))\n",
        "                self.times['TnT'].append(time.time() - time_before)\n",
        "\n",
        "            #  Perceptron tagger\n",
        "            if 'PER' in self.models:\n",
        "                time_before = time.time()\n",
        "                PER = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
        "                PER.train(train_data)\n",
        "                self.total_results['PER'].append(round(PER.accuracy(test_data), 3))\n",
        "                self.times['PER'].append(time.time() - time_before)\n",
        "\n",
        "            # Conditional Random Fields\n",
        "            if 'CRF' in self.models:\n",
        "                time_before = time.time()\n",
        "                CRF = nltk.tag.CRFTagger()\n",
        "                CRF.train(train_data,'crf_tagger_model')\n",
        "                self.total_results['CRF'].append(round(CRF.accuracy(test_data), 3))\n",
        "                self.times['CRF'].append(time.time() - time_before)\n",
        "\n",
        "            print(i)\n",
        "\n",
        "        return self.times, self.total_results\n",
        "    \n",
        "    def results(self):\n",
        "        df = pd.DataFrame.from_dict(self.total_results)\n",
        "\n",
        "        if 'HMM' in self.models:\n",
        "            plt.plot(self.amount_data, 'HMM', data=df, marker='.')\n",
        "        if 'TnT' in self.models:\n",
        "            plt.plot(self.amount_data, 'TnT', data=df, marker='.')\n",
        "        if 'PER' in self.models:\n",
        "            plt.plot(self.amount_data, 'PER', data=df, marker='.', markersize = 10)\n",
        "        if 'CRF' in self.models:\n",
        "            plt.plot(self.amount_data, 'CRF', data=df, marker='.')\n",
        "\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        df_times = pd.DataFrame.from_dict(self.times).round(3)\n",
        "        df_times['Sentences'] = self.amount_data\n",
        "        print(df_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw5wRtYzSsqf"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQRjabnaSsqf"
      },
      "source": [
        "## Loading the Training and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfZ9nHcASsqf"
      },
      "outputs": [],
      "source": [
        "# --------------- Training Data\n",
        "europarl_Train = pd.read_csv('Complementary Material/train/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "europarl_Train['gs'] = pd.read_csv('Complementary Material/train/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "\n",
        "vid_Train = pd.read_csv('Complementary Material/train/STS.input.MSRvid.txt',sep='\\t',header=None)\n",
        "vid_Train['gs'] = pd.read_csv('Complementary Material/train/STS.gs.MSRvid.txt',sep='\\t',header=None)\n",
        "\n",
        "with open('Complementary Material/train/STS.input.MSRpar.txt') as f:\n",
        "    lines = f.readlines()\n",
        "for index in range(len(lines)):\n",
        "    lines[index] = lines[index].replace('\\\"', ' ')\n",
        "par_Train = pd.read_csv(io.StringIO(''.join(lines)), sep='\\t',header=None, on_bad_lines='warn')\n",
        "par_Train['gs'] = pd.read_csv('Complementary Material/train/STS.gs.MSRpar.txt',sep='\\t',header=None)\n",
        "\n",
        "total_Train = pd.concat([europarl_Train, vid_Train, par_Train]).reset_index(drop=True)\n",
        "\n",
        "# --------------- Testing Data\n",
        "europarl_Test = pd.read_csv('Complementary Material/test-gold/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "europarl_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "\n",
        "vid_Test = pd.read_csv('Complementary Material/test-gold/STS.input.MSRvid.txt',sep='\\t',header=None)\n",
        "vid_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.MSRvid.txt',sep='\\t',header=None)\n",
        "\n",
        "with open('Complementary Material/test-gold/STS.input.MSRpar.txt', encoding='utf8') as f:\n",
        "    lines = f.readlines()\n",
        "for index in range(len(lines)):\n",
        "    lines[index] = lines[index].replace('\\\"', ' ')\n",
        "par_Test = pd.read_csv(io.StringIO(''.join(lines)), sep='\\t',header=None, on_bad_lines='warn')\n",
        "par_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.MSRpar.txt',sep='\\t',header=None)\n",
        "\n",
        "onwn_Test = pd.read_csv('Complementary Material/test-gold/STS.input.surprise.OnWN.txt',sep='\\t',header=None)\n",
        "onwn_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.surprise.OnWN.txt',sep='\\t',header=None)\n",
        "\n",
        "news_Test = pd.read_csv('Complementary Material/test-gold/STS.input.surprise.SMTnews.txt',sep='\\t',header=None)\n",
        "news_Test['gs'] = pd.read_csv('Complementary Material/test-gold/STS.gs.surprise.SMTnews.txt',sep='\\t',header=None) \n",
        "\n",
        "total_Test = pd.concat([europarl_Test, vid_Test, par_Test, onwn_Test, news_Test]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zs390sVSsqg"
      },
      "source": [
        "## Similarity measure computation\n",
        "In the next code cell we compute different similarity measures and store them in npy files in order to facilitate the the model fitting phase by using different subsets of extracted features. By changing the variable ``feature`` a similarity will be computed for the training and test data and stored in its own file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3fC0wj5Ssqg"
      },
      "outputs": [],
      "source": [
        "feature = 'trigram'\n",
        "\n",
        "Train1 = text_processing(total_Train[0])\n",
        "Train2 = text_processing(total_Train[1])\n",
        "#Train1.clean_data(auto=False, lowercase = lowercase, signs = signs),Train2.clean_data(auto=False, lowercase = lowercase, signs = signs)\n",
        "Train1.clean_data(),Train2.clean_data()\n",
        "_,_ = Train1.lemmatize_data(), Train2.lemmatize_data() \n",
        "\n",
        "save_train_path = 'Complementary Material/Extracted Features/Train/'+feature+'_comp_clean'\n",
        "X = compute_metrics([Train1.cleaned_data,Train2.cleaned_data],[feature]).do(save_train_path)\n",
        "\n",
        "Test1 = text_processing(total_Test[0])\n",
        "Test2 = text_processing(total_Test[1])\n",
        "#Test1.clean_data(auto=False, lowercase = lowercase, signs = signs),Test2.clean_data(auto=False, lowercase = lowercase, signs = signs)\n",
        "Test1.clean_data(),Test2.clean_data()\n",
        "_,_ = Test1.lemmatize_data(), Test2.lemmatize_data() \n",
        "\n",
        "save_test_path = 'Complementary Material/Extracted Features/Test/'+feature+'_comp_clean'\n",
        "X_test = compute_metrics([Test1.cleaned_data,Test2.cleaned_data],[feature]).do(save_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebDXYOkvSsqg"
      },
      "source": [
        "# Classification, Inference & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdI9pd-vSsqg"
      },
      "source": [
        "## Support Vector Regresor\n",
        "Fitting a SVR with a specific subset of features, and tunning the model parameters we got ~0.7406 in the pearson correlation between our predictions and the gold-standard la labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lysaStEkSsqg",
        "outputId": "ba49dfeb-e907-4f65-f8dd-9b382cda9907"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=0.7406540348318249, pvalue=0.0)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'bigram_semi_clean', 'trigram_semi_clean', 'jaccard_comp_clean', 'unigram_comp_clean', 'bigram_comp_clean']\n",
        "train_feats = np.load('Complementary Material/Extracted Features/Train/'+features_to_try[0]+'.npy')\n",
        "for n in range(1, len(features_to_try)):\n",
        "    train_feats = np.concatenate((train_feats, np.load('Complementary Material/Extracted Features/Train/'+features_to_try[n]+'.npy')))\n",
        "train_feats = np.moveaxis(np.array(train_feats),0,-1)\n",
        "\n",
        "y = total_Train['gs'].values\n",
        "\n",
        "regr = make_pipeline(StandardScaler(), SVR(kernel = 'rbf', C=1, epsilon=0, gamma = 'scale', tol=1e-1))\n",
        "regr.fit(train_feats, y)\n",
        "\n",
        "test_feats = np.load('Complementary Material/Extracted Features/Test/'+features_to_try[0]+'.npy')\n",
        "for n in range(1, len(features_to_try)):\n",
        "    test_feats = np.concatenate((test_feats, np.load('Complementary Material/Extracted Features/Test/'+features_to_try[n]+'.npy')))\n",
        "test_feats = np.moveaxis(np.array(test_feats),0,-1)\n",
        "\n",
        "pearsonr(total_Test['gs'].values,regr.predict(test_feats))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvkLzdDkSsqh"
      },
      "source": [
        "## Gradient Boosting Regresor\n",
        "Fitting a BGR with a specific subset of features, and tunning the model parameters we got ~0.7384 in the pearson correlation between our predictions and the gold-standard la labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8RxxDHvSsqh",
        "outputId": "4c897551-e925-4c96-d1a0-4275af23a42e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=0.7383876997011208, pvalue=0.0)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'bigram_semi_clean', 'trigram_semi_clean', 'jaccard_comp_clean', 'unigram_comp_clean', 'bigram_comp_clean']\n",
        "#features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'jaccard_comp_clean', 'unigram_comp_clean']\n",
        "train_feats = np.load('Complementary Material/Extracted Features/Train/'+features_to_try[0]+'.npy')\n",
        "for n in range(1, len(features_to_try)):\n",
        "    train_feats = np.concatenate((train_feats, np.load('Complementary Material/Extracted Features/Train/'+features_to_try[n]+'.npy')))\n",
        "train_feats = np.moveaxis(np.array(train_feats),0,-1)\n",
        "\n",
        "y = total_Train['gs'].values\n",
        "\n",
        "gbr = GradientBoostingRegressor(random_state=17)\n",
        "gbr.fit(train_feats, y)\n",
        "\n",
        "\n",
        "test_feats = np.load('Complementary Material/Extracted Features/Test/'+features_to_try[0]+'.npy')\n",
        "for n in range(1, len(features_to_try)):\n",
        "    test_feats = np.concatenate((test_feats, np.load('Complementary Material/Extracted Features/Test/'+features_to_try[n]+'.npy')))\n",
        "test_feats = np.moveaxis(np.array(test_feats),0,-1)\n",
        "\n",
        "pearsonr(total_Test['gs'].values,gbr.predict(test_feats))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPGX8dBvSsqi"
      },
      "source": [
        "## Multi-layer Perceptron Regresor\n",
        "Fitting a MLP-R with a specific subset of features, and tunning the model parameters we got ~0.7518 in the pearson correlation btween our predictions and the gold-standard la labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cum8DugeSsqi",
        "outputId": "36744ba4-6503-4ac6-dfc2-034afa736c2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PearsonRResult(statistic=0.7518254148216057, pvalue=0.0)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'bigram_semi_clean', 'trigram_semi_clean', 'jaccard_comp_clean', 'unigram_comp_clean', 'bigram_comp_clean']\n",
        "#features_to_try = ['LinSym_lemmas', 'LchSym_lemmas', 'PathSym_lemmas', 'jaccard_lemmas', 'cosine_lemmas', 'unigram_lemmas', 'bigram_lemmas', 'trigram_lemmas', 'jaccard_semi_clean', 'bigram_semi_clean', 'trigram_semi_clean']\n",
        "train_feats = np.load('Complementary Material/Extracted Features/Train/'+features_to_try[0]+'.npy', allow_pickle=True)\n",
        "for n in range(1, len(features_to_try)):\n",
        "    train_feats = np.concatenate((train_feats, np.load('Complementary Material/Extracted Features/Train/'+features_to_try[n]+'.npy')))\n",
        "train_feats = np.moveaxis(np.array(train_feats),0,-1)\n",
        "\n",
        "y = total_Train['gs'].values\n",
        "\n",
        "#mlp = MLPRegressor(hidden_layer_sizes=(100,500,1000,500,100), random_state=5, max_iter=900)\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(100,100,100, 100), random_state=3, max_iter=200)\n",
        "mlp.fit(train_feats, y)\n",
        "\n",
        "test_feats = np.load('Complementary Material/Extracted Features/Test/'+features_to_try[0]+'.npy', allow_pickle=True)\n",
        "for n in range(1, len(features_to_try)):\n",
        "    test_feats = np.concatenate((test_feats, np.load('Complementary Material/Extracted Features/Test/'+features_to_try[n]+'.npy')))\n",
        "test_feats = np.moveaxis(np.array(test_feats),0,-1)\n",
        "\n",
        "pearsonr(total_Test['gs'].values,mlp.predict(test_feats))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Result\n",
        "Using the multilayer perceptron we got 0.7518254148216057, this result was obtained with appropriate preprocessing, data extraction and model configuration. "
      ],
      "metadata": {
        "id": "OpJCe-vzTbvt"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('IHLT')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0795eca24a98e58b2dcbec80c9554a91f94c5c7d4e675f06c8c2f85c434623a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}